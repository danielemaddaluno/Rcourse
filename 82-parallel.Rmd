# Parallel Computing {#parallel}

You would think that because you have an expensive multicore computer your computations will speed up. 
Well, no.
At least not if you don't make sure they do.
By default, no matter how many cores you have, the operating system will allocate each R session to a single core. 

For starters, we need to distinguish between two types of parallelism:

1. __Explicit parallelism__: where the user handles the parallelisation. 
1. __Implicit parallelism__: where the parallelisation is abstracted away from the user.

Clearly, implicit parallelism is more desirable, but the state of mathematical computing is such that no sufficiently general implicit parallelism framework exists. 
The [R Consortium](https://www.r-consortium.org/projects/awarded-projects) is currently financing a major project for a A "Unified Framework For Distributed Computing in R" so we can expect things to change soon. 
In the meanwhile, most of the parallel implementations are explicit. 



## Implicit Parallelism

We will not elaborate on implicit parallelism except mentioning the following:

- You can enjoy parallel linear algebra by replacing the linear algebra libraries with BLAS and LAPACK as described [here](https://www.r-bloggers.com/faster-r-through-better-blas/).
- You should read the "Parallel computing: Implicit parallelism" section in the excellent [High Performance Computing](https://cran.r-project.org/web/views/HighPerformanceComputing.html) task view, for the latest developments in implicit parallelism.



## Explicit Parallelism

R provides many frameworks for explicit parallelism.
Because the parallelism is initiated by the user, we first need to decide __when to parallelize?__
As a rule of thumb, you want to parallelise when you encounter a CPU bottleneck, and not a memory bottleneck.
Memory bottlenecks are released with sparsity (Chapter \@ref(sparse)), or efficient memory usage (Chapter \@ref(memory)).

Several ways to diagnose your bottleneck include:

- Keep your Windows Task Manager, or Linux `top` open, and look for the CPU load, and RAM loads. 
- The computation takes a long time, and when you stop it pressing ESC, R is immediately responsive. If it is not immediately responsive, you have a memory bottleneck.
- Profile your code. See Hadley's [guide](http://adv-r.had.co.nz/Profiling.html).

For reasons detailed in @kane2013scalable, we will present the __foreach__ parallelisation package [@foreach]. 
It will allow us to: 

1. Decouple between our parallel algorithm and the parallelisation mechanism: we write parallelisable code once, and can then switch the underlying parallelisation mechanism. 

1. Combine with the `big.matrix` object from Chapter \@ref(memory) for _shared memory parallisation_: all the machines may see the same data, so that we don't need to export objects from machine to machine. 


What do we mean by "switch the underlying parallesation mechanism"? 
It means there are several packages that will handle communication between machines. 
Some are very general and will work on any cluster. 
Some are more specific and will work only on a single multicore machine (not a cluster) with a particular operating system.
These mechanisms include __multicore__, __snow__, __parallel__, and __Rmpi__.
The compatibility between these mechanisms and __foreach__ is provided by another set of packages:
__doMC__ , __doMPI__, __doRedis__, __doParallel__, and __doSNOW__.


```{remark}
I personally prefer the __multicore__ mechanism, with the __doMC__ adapter for __foreach__.
I will not use this combo, however, because __multicore__ will not work on Windows machines.
I will thus use the more general __snow__ and __doParallel__ combo. 
If you do happen to run on Linux, or Unix, you will want to replace all __doParallel__ functionality with __doMC__.
```


Let's start with a simple example, taken from ["Getting Started with doParallel and foreach"](http://debian.mc.vanderbilt.edu/R/CRAN/web/packages/doParallel/vignettes/gettingstartedParallel.pdf).

```{r}
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)
result <- foreach(i=1:3) %dopar% sqrt(i)
class(result)
result
```

Things to note:

- `makeCluster` creates an object with the information our cluster. 
On a single machine it is very simple. On a cluster of machines, you will need to specify the i.p. addresses or other identifiers of the machines. 
- `registerDoParallel` is used to inform the __foreach__ package of the presence of our cluster. 
- The `foreach` function handles the looping. In particular note the `%dopar` operator that ensures that looping is in parallel. `%dopar%` can be replaced by `%do%` if you want serial looping (like the `for` loop), for instance, for debugging. 
- The output of the various machines is collected by `foreach` to a list object. 
- In this simple example, no data is shared between machines so we are not putting the shared memory capabilities to the test. 
- We can check how many workers were involved using the `getDoParWorkers()` function.
- We can check the parallelisation mechanism used with the `getDoParName()` function.


Here is a more involved example.
We now try to make [Bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) inference on the coefficients of a logistic regression.
Bootstrapping means that in each iteration, we resample the data, and refit the model. 

```{r}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- 1e4
ptime <- system.time({
 r <- foreach(icount(trials), .combine=cbind) %dopar% {
 ind <- sample(100, 100, replace=TRUE)
 result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
 coefficients(result1)
 }
 })[3]
ptime
```

Things to note:

- As usual, we use the `foreach` function with the `%dopar%` operator to loop in parallel.
- The `icounts` function generates a counter. 
- The `.combine=cbind` argument tells the `foreach` function how to combine the output of different machines, so that the returned object is not the default list. 


How long would that have taken in a simple (serial) loop? 
We only need to replace `%dopar%` with `%do%` to test.

```{r}
stime <- system.time({
 r <- foreach(icount(trials), .combine=cbind) %do% {
 ind <- sample(100, 100, replace=TRUE)
 result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
 coefficients(result1)
 }
 })[3]
stime
```

Yes. Parallelising is clearly faster.


Let's see how we can combine the power of __bigmemory__ and __foreach__ by creating a file mapped `big.matrix` object, which is shared by all machines.
The following example is taken from @kane2013scalable, and uses the `big.matrix` object we created in Chapter \@ref(memory).

```{r}
library(bigmemory)
x <- attach.big.matrix("airline.desc")

library(foreach)
library(doSNOW)
cl <- makeSOCKcluster(rep("localhost", 4)) # make a cluster of 4 machines
registerDoSNOW(cl) # register machines for foreach()
```

Get a "description" of the `big.matrix` object that will be used to call it from each machine.
```{r}
xdesc <- describe(x) 
```

Split the data along values of `BENE_AGE_CAT_CD`.
```{r}
G <- split(1:nrow(x), x[, "BENE_AGE_CAT_CD"]) 
```

Define a function that computes quantiles of `CAR_LINE_ICD9_DGNS_CD`.
```{r}
GetDepQuantiles <- function(rows, data) {
 quantile(data[rows, "CAR_LINE_ICD9_DGNS_CD"], probs = c(0.5, 0.9, 0.99),
 na.rm = TRUE)
}
```

We are all set up to loop, in parallel, and compute quantiles of `CAR_LINE_ICD9_DGNS_CD` for each value of `BENE_AGE_CAT_CD`.

```{r}
qs <- foreach(g = G, .combine = rbind) %dopar% {
 require("bigmemory")
 x <- attach.big.matrix(xdesc)
 GetDepQuantiles(rows = g, data = x)
}
qs
```






### Caution: Implicit with Explicit Parallelism
A common problem when parallelizing is that the machines you invoked with __explicit__ parallelism, invoke other machines using __implicit__ parallelism. 
You then lose control of the number of machine being invoked, and the operating system spends most of its time managing resources, instead of doing your computations. 

Modern linear algebra libraries may try to implicitly parallelize vector and matrix operations.
To avoid this type of implicity parallelism within an explicit paralleliation, use the [RhpcBLASctl package](), and in particular  _evaRhpcBLASctl::blas_set_num_threads()_.
In other cases, you should consult an expert. 





## Bibliographic Notes
For a brief and excellent explanation on parallel computing in R see @schmidberger2009state.
For a full review see @chapple2016mastering.
For a blog-level introduction see [ParallelR
](http://www.parallelr.com/r-with-parallel-computing/).
For an up-to-date list of packages supporting parallel programming see the High Performance Computing [R task view](https://cran.r-project.org/web/views/HighPerformanceComputing.html).

## Practice Yourself

Try DataCamp's [Parallel Programming in R](https://www.datacamp.com/courses/parallel-programming-in-r).
