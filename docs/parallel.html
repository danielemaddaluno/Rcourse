<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Parallel Computing | R (BGU course)</title>
  <meta name="description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Parallel Computing | R (BGU course)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Parallel Computing | R (BGU course)" />
  
  <meta name="twitter:description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  

<meta name="author" content="Jonathan D. Rosenblatt" />


<meta name="date" content="2019-10-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="memory.html">
<link rel="next" href="algebra.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<link href="libs/sequences-0.1/sequences.css" rel="stylesheet" />
<script src="libs/sunburst-binding-2.1.1/sunburst.js"></script>
<script src="libs/d3-5.7.0/d3.min.js"></script>
<script src="libs/d3-lasso-0.0.5/d3-lasso.min.js"></script>
<link href="libs/ggiraphjs-0.1.0/styles.css" rel="stylesheet" />
<script src="libs/ggiraphjs-0.1.0/ggiraphjs.min.js"></script>
<script src="libs/girafe-binding-0.6.1/girafe.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Course</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#notation-conventions"><i class="fa fa-check"></i><b>1.1</b> Notation Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#ecosystem"><i class="fa fa-check"></i><b>2.2</b> The R Ecosystem</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#bibliographic-notes"><i class="fa fa-check"></i><b>2.3</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>3</b> R Basics</a><ul>
<li class="chapter" data-level="3.0.1" data-path="basics.html"><a href="basics.html#other-ides"><i class="fa fa-check"></i><b>3.0.1</b> Other IDEs</a></li>
<li class="chapter" data-level="3.1" data-path="basics.html"><a href="basics.html#file-types"><i class="fa fa-check"></i><b>3.1</b> File types</a></li>
<li class="chapter" data-level="3.2" data-path="basics.html"><a href="basics.html#simple-calculator"><i class="fa fa-check"></i><b>3.2</b> Simple calculator</a></li>
<li class="chapter" data-level="3.3" data-path="basics.html"><a href="basics.html#probability-calculator"><i class="fa fa-check"></i><b>3.3</b> Probability calculator</a></li>
<li class="chapter" data-level="3.4" data-path="basics.html"><a href="basics.html#getting-help"><i class="fa fa-check"></i><b>3.4</b> Getting Help</a></li>
<li class="chapter" data-level="3.5" data-path="basics.html"><a href="basics.html#variable-assignment"><i class="fa fa-check"></i><b>3.5</b> Variable Assignment</a></li>
<li class="chapter" data-level="3.6" data-path="basics.html"><a href="basics.html#missing"><i class="fa fa-check"></i><b>3.6</b> Missing</a></li>
<li class="chapter" data-level="3.7" data-path="basics.html"><a href="basics.html#piping"><i class="fa fa-check"></i><b>3.7</b> Piping</a></li>
<li class="chapter" data-level="3.8" data-path="basics.html"><a href="basics.html#vector-creation-and-manipulation"><i class="fa fa-check"></i><b>3.8</b> Vector Creation and Manipulation</a></li>
<li class="chapter" data-level="3.9" data-path="basics.html"><a href="basics.html#search-paths-and-packages"><i class="fa fa-check"></i><b>3.9</b> Search Paths and Packages</a></li>
<li class="chapter" data-level="3.10" data-path="basics.html"><a href="basics.html#simple-plotting"><i class="fa fa-check"></i><b>3.10</b> Simple Plotting</a></li>
<li class="chapter" data-level="3.11" data-path="basics.html"><a href="basics.html#object-types"><i class="fa fa-check"></i><b>3.11</b> Object Types</a></li>
<li class="chapter" data-level="3.12" data-path="basics.html"><a href="basics.html#data-frames"><i class="fa fa-check"></i><b>3.12</b> Data Frames</a></li>
<li class="chapter" data-level="3.13" data-path="basics.html"><a href="basics.html#exctraction"><i class="fa fa-check"></i><b>3.13</b> Exctraction</a></li>
<li class="chapter" data-level="3.14" data-path="basics.html"><a href="basics.html#augmentations-of-the-data.frame-class"><i class="fa fa-check"></i><b>3.14</b> Augmentations of the data.frame class</a></li>
<li class="chapter" data-level="3.15" data-path="basics.html"><a href="basics.html#data-import-and-export"><i class="fa fa-check"></i><b>3.15</b> Data Import and Export</a><ul>
<li class="chapter" data-level="3.15.1" data-path="basics.html"><a href="basics.html#import-from-web"><i class="fa fa-check"></i><b>3.15.1</b> Import from WEB</a></li>
<li class="chapter" data-level="3.15.2" data-path="basics.html"><a href="basics.html#import-from-clipboard"><i class="fa fa-check"></i><b>3.15.2</b> Import From Clipboard</a></li>
<li class="chapter" data-level="3.15.3" data-path="basics.html"><a href="basics.html#export-as-csv"><i class="fa fa-check"></i><b>3.15.3</b> Export as CSV</a></li>
<li class="chapter" data-level="3.15.4" data-path="basics.html"><a href="basics.html#export-non-csv-files"><i class="fa fa-check"></i><b>3.15.4</b> Export non-CSV files</a></li>
<li class="chapter" data-level="3.15.5" data-path="basics.html"><a href="basics.html#reading-from-text-files"><i class="fa fa-check"></i><b>3.15.5</b> Reading From Text Files</a></li>
<li class="chapter" data-level="3.15.6" data-path="basics.html"><a href="basics.html#writing-data-to-text-files"><i class="fa fa-check"></i><b>3.15.6</b> Writing Data to Text Files</a></li>
<li class="chapter" data-level="3.15.7" data-path="basics.html"><a href="basics.html#xlsx-files"><i class="fa fa-check"></i><b>3.15.7</b> .XLS(X) files</a></li>
<li class="chapter" data-level="3.15.8" data-path="basics.html"><a href="basics.html#massive-files"><i class="fa fa-check"></i><b>3.15.8</b> Massive files</a></li>
<li class="chapter" data-level="3.15.9" data-path="basics.html"><a href="basics.html#databases"><i class="fa fa-check"></i><b>3.15.9</b> Databases</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="basics.html"><a href="basics.html#functions"><i class="fa fa-check"></i><b>3.16</b> Functions</a></li>
<li class="chapter" data-level="3.17" data-path="basics.html"><a href="basics.html#looping"><i class="fa fa-check"></i><b>3.17</b> Looping</a></li>
<li class="chapter" data-level="3.18" data-path="basics.html"><a href="basics.html#apply"><i class="fa fa-check"></i><b>3.18</b> Apply</a></li>
<li class="chapter" data-level="3.19" data-path="basics.html"><a href="basics.html#recursion"><i class="fa fa-check"></i><b>3.19</b> Recursion</a></li>
<li class="chapter" data-level="3.20" data-path="basics.html"><a href="basics.html#strings"><i class="fa fa-check"></i><b>3.20</b> Strings</a></li>
<li class="chapter" data-level="3.21" data-path="basics.html"><a href="basics.html#dates-and-times"><i class="fa fa-check"></i><b>3.21</b> Dates and Times</a><ul>
<li class="chapter" data-level="3.21.1" data-path="basics.html"><a href="basics.html#dates"><i class="fa fa-check"></i><b>3.21.1</b> Dates</a></li>
<li class="chapter" data-level="3.21.2" data-path="basics.html"><a href="basics.html#times"><i class="fa fa-check"></i><b>3.21.2</b> Times</a></li>
<li class="chapter" data-level="3.21.3" data-path="basics.html"><a href="basics.html#lubridate-package"><i class="fa fa-check"></i><b>3.21.3</b> lubridate Package</a></li>
</ul></li>
<li class="chapter" data-level="3.22" data-path="basics.html"><a href="basics.html#complex-objects"><i class="fa fa-check"></i><b>3.22</b> Complex Objects</a></li>
<li class="chapter" data-level="3.23" data-path="basics.html"><a href="basics.html#vectors-and-matrix-products"><i class="fa fa-check"></i><b>3.23</b> Vectors and Matrix Products</a></li>
<li class="chapter" data-level="3.24" data-path="basics.html"><a href="basics.html#rstudio-projects"><i class="fa fa-check"></i><b>3.24</b> RStudio Projects</a></li>
<li class="chapter" data-level="3.25" data-path="basics.html"><a href="basics.html#bibliographic-notes-1"><i class="fa fa-check"></i><b>3.25</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="3.26" data-path="basics.html"><a href="basics.html#practice-yourself"><i class="fa fa-check"></i><b>3.26</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="datatable.html"><a href="datatable.html"><i class="fa fa-check"></i><b>4</b> data.table</a><ul>
<li class="chapter" data-level="4.1" data-path="datatable.html"><a href="datatable.html#make-your-own-variables"><i class="fa fa-check"></i><b>4.1</b> Make your own variables</a></li>
<li class="chapter" data-level="4.2" data-path="datatable.html"><a href="datatable.html#join"><i class="fa fa-check"></i><b>4.2</b> Join</a></li>
<li class="chapter" data-level="4.3" data-path="datatable.html"><a href="datatable.html#reshaping-data"><i class="fa fa-check"></i><b>4.3</b> Reshaping data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="datatable.html"><a href="datatable.html#wide-to-long"><i class="fa fa-check"></i><b>4.3.1</b> Wide to long</a></li>
<li class="chapter" data-level="4.3.2" data-path="datatable.html"><a href="datatable.html#long-to-wide"><i class="fa fa-check"></i><b>4.3.2</b> Long to wide</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="datatable.html"><a href="datatable.html#bibliographic-notes-2"><i class="fa fa-check"></i><b>4.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="4.5" data-path="datatable.html"><a href="datatable.html#practice-yourself-1"><i class="fa fa-check"></i><b>4.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="eda.html"><a href="eda.html#summary-statistics"><i class="fa fa-check"></i><b>5.1</b> Summary Statistics</a><ul>
<li class="chapter" data-level="5.1.1" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>5.1.1</b> Categorical Data</a></li>
<li class="chapter" data-level="5.1.2" data-path="eda.html"><a href="eda.html#continous-data"><i class="fa fa-check"></i><b>5.1.2</b> Continous Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="eda.html"><a href="eda.html#visualization"><i class="fa fa-check"></i><b>5.2</b> Visualization</a><ul>
<li class="chapter" data-level="5.2.1" data-path="eda.html"><a href="eda.html#categorical-data-1"><i class="fa fa-check"></i><b>5.2.1</b> Categorical Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="eda.html"><a href="eda.html#continuous-data"><i class="fa fa-check"></i><b>5.2.2</b> Continuous Data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="eda.html"><a href="eda.html#mixed-type-data"><i class="fa fa-check"></i><b>5.3</b> Mixed Type Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="eda.html"><a href="eda.html#alluvial"><i class="fa fa-check"></i><b>5.3.1</b> Alluvial Diagram</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="eda.html"><a href="eda.html#bibliographic-notes-3"><i class="fa fa-check"></i><b>5.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="5.5" data-path="eda.html"><a href="eda.html#practice-yourself-2"><i class="fa fa-check"></i><b>5.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>6</b> Linear Models</a><ul>
<li class="chapter" data-level="6.1" data-path="lm.html"><a href="lm.html#problem-setup"><i class="fa fa-check"></i><b>6.1</b> Problem Setup</a></li>
<li class="chapter" data-level="6.2" data-path="lm.html"><a href="lm.html#ols-estimation-in-r"><i class="fa fa-check"></i><b>6.2</b> OLS Estimation in R</a></li>
<li class="chapter" data-level="6.3" data-path="lm.html"><a href="lm.html#inference"><i class="fa fa-check"></i><b>6.3</b> Inference</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-coefficient"><i class="fa fa-check"></i><b>6.3.1</b> Testing a Hypothesis on a Single Coefficient</a></li>
<li class="chapter" data-level="6.3.2" data-path="lm.html"><a href="lm.html#constructing-a-confidence-interval-on-a-single-coefficient"><i class="fa fa-check"></i><b>6.3.2</b> Constructing a Confidence Interval on a Single Coefficient</a></li>
<li class="chapter" data-level="6.3.3" data-path="lm.html"><a href="lm.html#multiple-regression"><i class="fa fa-check"></i><b>6.3.3</b> Multiple Regression</a></li>
<li class="chapter" data-level="6.3.4" data-path="lm.html"><a href="lm.html#anova"><i class="fa fa-check"></i><b>6.3.4</b> ANOVA (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-contrast"><i class="fa fa-check"></i><b>6.3.5</b> Testing a Hypothesis on a Single Contrast (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lm.html"><a href="lm.html#extra-diagnostics"><i class="fa fa-check"></i><b>6.4</b> Extra Diagnostics</a><ul>
<li class="chapter" data-level="6.4.1" data-path="lm.html"><a href="lm.html#diagnosing-heteroskedasticity"><i class="fa fa-check"></i><b>6.4.1</b> Diagnosing Heteroskedasticity</a></li>
<li class="chapter" data-level="6.4.2" data-path="lm.html"><a href="lm.html#diagnosing-multicolinearity"><i class="fa fa-check"></i><b>6.4.2</b> Diagnosing Multicolinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="lm.html"><a href="lm.html#bibliographic-notes-4"><i class="fa fa-check"></i><b>6.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="6.6" data-path="lm.html"><a href="lm.html#practice-yourself-3"><i class="fa fa-check"></i><b>6.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="glm.html"><a href="glm.html#problem-setup-1"><i class="fa fa-check"></i><b>7.1</b> Problem Setup</a></li>
<li class="chapter" data-level="7.2" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="glm.html"><a href="glm.html#logistic-regression-with-r"><i class="fa fa-check"></i><b>7.2.1</b> Logistic Regression with R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>7.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="7.4" data-path="glm.html"><a href="glm.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a></li>
<li class="chapter" data-level="7.5" data-path="glm.html"><a href="glm.html#bibliographic-notes-5"><i class="fa fa-check"></i><b>7.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="7.6" data-path="glm.html"><a href="glm.html#practice-glm"><i class="fa fa-check"></i><b>7.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lme.html"><a href="lme.html"><i class="fa fa-check"></i><b>8</b> Linear Mixed Models</a><ul>
<li class="chapter" data-level="8.1" data-path="lme.html"><a href="lme.html#problem-setup-2"><i class="fa fa-check"></i><b>8.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="8.1.1" data-path="lme.html"><a href="lme.html#non-linear-mixed-models"><i class="fa fa-check"></i><b>8.1.1</b> Non-Linear Mixed Models</a></li>
<li class="chapter" data-level="8.1.2" data-path="lme.html"><a href="lme.html#generalized-linear-mixed-models-glmm"><i class="fa fa-check"></i><b>8.1.2</b> Generalized Linear Mixed Models (GLMM)</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="lme.html"><a href="lme.html#mixed-models-with-r"><i class="fa fa-check"></i><b>8.2</b> Mixed Models with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="lme.html"><a href="lme.html#a-single-random-effect"><i class="fa fa-check"></i><b>8.2.1</b> A Single Random Effect</a></li>
<li class="chapter" data-level="8.2.2" data-path="lme.html"><a href="lme.html#multiple-random-effects"><i class="fa fa-check"></i><b>8.2.2</b> Multiple Random Effects</a></li>
<li class="chapter" data-level="8.2.3" data-path="lme.html"><a href="lme.html#a-full-mixed-model"><i class="fa fa-check"></i><b>8.2.3</b> A Full Mixed-Model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="lme.html"><a href="lme.html#serial"><i class="fa fa-check"></i><b>8.3</b> Serial Correlations</a></li>
<li class="chapter" data-level="8.4" data-path="lme.html"><a href="lme.html#extensions-1"><i class="fa fa-check"></i><b>8.4</b> Extensions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="lme.html"><a href="lme.html#cr-se"><i class="fa fa-check"></i><b>8.4.1</b> Cluster Robust Standard Errors</a></li>
<li class="chapter" data-level="8.4.2" data-path="lme.html"><a href="lme.html#linear-models-for-panel-data"><i class="fa fa-check"></i><b>8.4.2</b> Linear Models for Panel Data</a></li>
<li class="chapter" data-level="8.4.3" data-path="lme.html"><a href="lme.html#testing-hypotheses-on-correlations"><i class="fa fa-check"></i><b>8.4.3</b> Testing Hypotheses on Correlations</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lme.html"><a href="lme.html#relation-to-other-estimators"><i class="fa fa-check"></i><b>8.5</b> Relation to Other Estimators</a><ul>
<li class="chapter" data-level="8.5.1" data-path="lme.html"><a href="lme.html#fixed-effects-in-the-econometric-literature"><i class="fa fa-check"></i><b>8.5.1</b> Fixed Effects in the Econometric Literature</a></li>
<li class="chapter" data-level="8.5.2" data-path="lme.html"><a href="lme.html#relation-to-generalized-least-squares-gls"><i class="fa fa-check"></i><b>8.5.2</b> Relation to Generalized Least Squares (GLS)</a></li>
<li class="chapter" data-level="8.5.3" data-path="lme.html"><a href="lme.html#relation-to-conditional-gaussian-fields"><i class="fa fa-check"></i><b>8.5.3</b> Relation to Conditional Gaussian Fields</a></li>
<li class="chapter" data-level="8.5.4" data-path="lme.html"><a href="lme.html#relation-to-empirical-risk-minimization-erm"><i class="fa fa-check"></i><b>8.5.4</b> Relation to Empirical Risk Minimization (ERM)</a></li>
<li class="chapter" data-level="8.5.5" data-path="lme.html"><a href="lme.html#relation-to-m-estimation"><i class="fa fa-check"></i><b>8.5.5</b> Relation to M-Estimation</a></li>
<li class="chapter" data-level="8.5.6" data-path="lme.html"><a href="lme.html#relation-to-generalize-estimating-equations-gee"><i class="fa fa-check"></i><b>8.5.6</b> Relation to Generalize Estimating Equations (GEE)</a></li>
<li class="chapter" data-level="8.5.7" data-path="lme.html"><a href="lme.html#manova"><i class="fa fa-check"></i><b>8.5.7</b> Relation to MANOVA</a></li>
<li class="chapter" data-level="8.5.8" data-path="lme.html"><a href="lme.html#relation-to-seemingly-unrelated-equations-sur"><i class="fa fa-check"></i><b>8.5.8</b> Relation to Seemingly Unrelated Equations (SUR)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lme.html"><a href="lme.html#bibliographic-notes-6"><i class="fa fa-check"></i><b>8.6</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="8.7" data-path="lme.html"><a href="lme.html#practice-yourself-4"><i class="fa fa-check"></i><b>8.7</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multivariate.html"><a href="multivariate.html"><i class="fa fa-check"></i><b>9</b> Multivariate Data Analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="multivariate.html"><a href="multivariate.html#signal-detection"><i class="fa fa-check"></i><b>9.1</b> Signal Detection</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multivariate.html"><a href="multivariate.html#hotellings-t2-test"><i class="fa fa-check"></i><b>9.1.1</b> Hotelling’s T2 Test</a></li>
<li class="chapter" data-level="9.1.2" data-path="multivariate.html"><a href="multivariate.html#various-types-of-signal-to-detect"><i class="fa fa-check"></i><b>9.1.2</b> Various Types of Signal to Detect</a></li>
<li class="chapter" data-level="9.1.3" data-path="multivariate.html"><a href="multivariate.html#simes-test"><i class="fa fa-check"></i><b>9.1.3</b> Simes’ Test</a></li>
<li class="chapter" data-level="9.1.4" data-path="multivariate.html"><a href="multivariate.html#signal-detection-with-r"><i class="fa fa-check"></i><b>9.1.4</b> Signal Detection with R</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multivariate.html"><a href="multivariate.html#signal-counting"><i class="fa fa-check"></i><b>9.2</b> Signal Counting</a></li>
<li class="chapter" data-level="9.3" data-path="multivariate.html"><a href="multivariate.html#identification"><i class="fa fa-check"></i><b>9.3</b> Signal Identification</a><ul>
<li class="chapter" data-level="9.3.1" data-path="multivariate.html"><a href="multivariate.html#signal-identification-in-r"><i class="fa fa-check"></i><b>9.3.1</b> Signal Identification in R</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="multivariate.html"><a href="multivariate.html#signal-estimation"><i class="fa fa-check"></i><b>9.4</b> Signal Estimation (*)</a></li>
<li class="chapter" data-level="9.5" data-path="multivariate.html"><a href="multivariate.html#bibliographic-notes-7"><i class="fa fa-check"></i><b>9.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="9.6" data-path="multivariate.html"><a href="multivariate.html#practice-yourself-5"><i class="fa fa-check"></i><b>9.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised.html"><a href="supervised.html"><i class="fa fa-check"></i><b>10</b> Supervised Learning</a><ul>
<li class="chapter" data-level="10.1" data-path="supervised.html"><a href="supervised.html#problem-setup-3"><i class="fa fa-check"></i><b>10.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="10.1.1" data-path="supervised.html"><a href="supervised.html#common-hypothesis-classes"><i class="fa fa-check"></i><b>10.1.1</b> Common Hypothesis Classes</a></li>
<li class="chapter" data-level="10.1.2" data-path="supervised.html"><a href="supervised.html#common-complexity-penalties"><i class="fa fa-check"></i><b>10.1.2</b> Common Complexity Penalties</a></li>
<li class="chapter" data-level="10.1.3" data-path="supervised.html"><a href="supervised.html#unbiased-risk-estimation"><i class="fa fa-check"></i><b>10.1.3</b> Unbiased Risk Estimation</a></li>
<li class="chapter" data-level="10.1.4" data-path="supervised.html"><a href="supervised.html#collecting-the-pieces"><i class="fa fa-check"></i><b>10.1.4</b> Collecting the Pieces</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="supervised.html"><a href="supervised.html#supervised-learning-in-r"><i class="fa fa-check"></i><b>10.2</b> Supervised Learning in R</a><ul>
<li class="chapter" data-level="10.2.1" data-path="supervised.html"><a href="supervised.html#least-squares"><i class="fa fa-check"></i><b>10.2.1</b> Linear Models with Least Squares Loss</a></li>
<li class="chapter" data-level="10.2.2" data-path="supervised.html"><a href="supervised.html#svm"><i class="fa fa-check"></i><b>10.2.2</b> SVM</a></li>
<li class="chapter" data-level="10.2.3" data-path="supervised.html"><a href="supervised.html#neural-nets"><i class="fa fa-check"></i><b>10.2.3</b> Neural Nets</a></li>
<li class="chapter" data-level="10.2.4" data-path="supervised.html"><a href="supervised.html#trees"><i class="fa fa-check"></i><b>10.2.4</b> Classification and Regression Trees (CART)</a></li>
<li class="chapter" data-level="10.2.5" data-path="supervised.html"><a href="supervised.html#k-nearest-neighbour-knn"><i class="fa fa-check"></i><b>10.2.5</b> K-nearest neighbour (KNN)</a></li>
<li class="chapter" data-level="10.2.6" data-path="supervised.html"><a href="supervised.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>10.2.6</b> Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="10.2.7" data-path="supervised.html"><a href="supervised.html#naive-bayes"><i class="fa fa-check"></i><b>10.2.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.2.8" data-path="supervised.html"><a href="supervised.html#random-forrest"><i class="fa fa-check"></i><b>10.2.8</b> Random Forrest</a></li>
<li class="chapter" data-level="10.2.9" data-path="supervised.html"><a href="supervised.html#boosting"><i class="fa fa-check"></i><b>10.2.9</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="supervised.html"><a href="supervised.html#bibliographic-notes-8"><i class="fa fa-check"></i><b>10.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="10.4" data-path="supervised.html"><a href="supervised.html#practice-yourself-6"><i class="fa fa-check"></i><b>10.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="11.1" data-path="unsupervised.html"><a href="unsupervised.html#dim-reduce"><i class="fa fa-check"></i><b>11.1</b> Dimensionality Reduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="unsupervised.html"><a href="unsupervised.html#pca"><i class="fa fa-check"></i><b>11.1.1</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="unsupervised.html"><a href="unsupervised.html#dimensionality-reduction-preliminaries"><i class="fa fa-check"></i><b>11.1.2</b> Dimensionality Reduction Preliminaries</a></li>
<li class="chapter" data-level="11.1.3" data-path="unsupervised.html"><a href="unsupervised.html#latent-variable-generative-approaches"><i class="fa fa-check"></i><b>11.1.3</b> Latent Variable Generative Approaches</a></li>
<li class="chapter" data-level="11.1.4" data-path="unsupervised.html"><a href="unsupervised.html#purely-algorithmic-approaches"><i class="fa fa-check"></i><b>11.1.4</b> Purely Algorithmic Approaches</a></li>
<li class="chapter" data-level="11.1.5" data-path="unsupervised.html"><a href="unsupervised.html#dimensionality-reduction-in-r"><i class="fa fa-check"></i><b>11.1.5</b> Dimensionality Reduction in R</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="unsupervised.html"><a href="unsupervised.html#cluster"><i class="fa fa-check"></i><b>11.2</b> Clustering</a><ul>
<li class="chapter" data-level="11.2.1" data-path="unsupervised.html"><a href="unsupervised.html#latent-variable-generative-approaches-1"><i class="fa fa-check"></i><b>11.2.1</b> Latent Variable Generative Approaches</a></li>
<li class="chapter" data-level="11.2.2" data-path="unsupervised.html"><a href="unsupervised.html#purely-algorithmic-approaches-1"><i class="fa fa-check"></i><b>11.2.2</b> Purely Algorithmic Approaches</a></li>
<li class="chapter" data-level="11.2.3" data-path="unsupervised.html"><a href="unsupervised.html#clustering-in-r"><i class="fa fa-check"></i><b>11.2.3</b> Clustering in R</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="unsupervised.html"><a href="unsupervised.html#bibliographic-notes-9"><i class="fa fa-check"></i><b>11.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="11.4" data-path="unsupervised.html"><a href="unsupervised.html#practice-yourself-7"><i class="fa fa-check"></i><b>11.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="plotting.html"><a href="plotting.html"><i class="fa fa-check"></i><b>12</b> Plotting</a><ul>
<li class="chapter" data-level="12.1" data-path="plotting.html"><a href="plotting.html#the-graphics-system"><i class="fa fa-check"></i><b>12.1</b> The graphics System</a><ul>
<li class="chapter" data-level="12.1.1" data-path="plotting.html"><a href="plotting.html#using-existing-plotting-functions"><i class="fa fa-check"></i><b>12.1.1</b> Using Existing Plotting Functions</a></li>
<li class="chapter" data-level="12.1.2" data-path="plotting.html"><a href="plotting.html#exporting-a-plot"><i class="fa fa-check"></i><b>12.1.2</b> Exporting a Plot</a></li>
<li class="chapter" data-level="12.1.3" data-path="plotting.html"><a href="plotting.html#fancy"><i class="fa fa-check"></i><b>12.1.3</b> Fancy graphics Examples</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="plotting.html"><a href="plotting.html#the-ggplot2-system"><i class="fa fa-check"></i><b>12.2</b> The ggplot2 System</a><ul>
<li class="chapter" data-level="12.2.1" data-path="plotting.html"><a href="plotting.html#extensions-of-the-ggplot2-system"><i class="fa fa-check"></i><b>12.2.1</b> Extensions of the ggplot2 System</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="plotting.html"><a href="plotting.html#interactive-graphics"><i class="fa fa-check"></i><b>12.3</b> Interactive Graphics</a><ul>
<li class="chapter" data-level="12.3.1" data-path="plotting.html"><a href="plotting.html#plotly"><i class="fa fa-check"></i><b>12.3.1</b> Plotly</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="plotting.html"><a href="plotting.html#other-r-interfaces-to-javascript-plotting"><i class="fa fa-check"></i><b>12.4</b> Other R Interfaces to JavaScript Plotting</a></li>
<li class="chapter" data-level="12.5" data-path="plotting.html"><a href="plotting.html#bibliographic-notes-10"><i class="fa fa-check"></i><b>12.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="12.6" data-path="plotting.html"><a href="plotting.html#practice-yourself-8"><i class="fa fa-check"></i><b>12.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>13</b> Reports</a><ul>
<li class="chapter" data-level="13.1" data-path="report.html"><a href="report.html#knitr"><i class="fa fa-check"></i><b>13.1</b> knitr</a><ul>
<li class="chapter" data-level="13.1.1" data-path="report.html"><a href="report.html#installation"><i class="fa fa-check"></i><b>13.1.1</b> Installation</a></li>
<li class="chapter" data-level="13.1.2" data-path="report.html"><a href="report.html#pandoc-markdown"><i class="fa fa-check"></i><b>13.1.2</b> Pandoc Markdown</a></li>
<li class="chapter" data-level="13.1.3" data-path="report.html"><a href="report.html#rmarkdown"><i class="fa fa-check"></i><b>13.1.3</b> Rmarkdown</a></li>
<li class="chapter" data-level="13.1.4" data-path="report.html"><a href="report.html#bibtex"><i class="fa fa-check"></i><b>13.1.4</b> BibTex</a></li>
<li class="chapter" data-level="13.1.5" data-path="report.html"><a href="report.html#compiling"><i class="fa fa-check"></i><b>13.1.5</b> Compiling</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="report.html"><a href="report.html#bookdown"><i class="fa fa-check"></i><b>13.2</b> bookdown</a></li>
<li class="chapter" data-level="13.3" data-path="report.html"><a href="report.html#shiny"><i class="fa fa-check"></i><b>13.3</b> Shiny</a><ul>
<li class="chapter" data-level="13.3.1" data-path="report.html"><a href="report.html#installation-1"><i class="fa fa-check"></i><b>13.3.1</b> Installation</a></li>
<li class="chapter" data-level="13.3.2" data-path="report.html"><a href="report.html#the-basics-of-shiny"><i class="fa fa-check"></i><b>13.3.2</b> The Basics of Shiny</a></li>
<li class="chapter" data-level="13.3.3" data-path="report.html"><a href="report.html#beyond-the-basics"><i class="fa fa-check"></i><b>13.3.3</b> Beyond the Basics</a></li>
<li class="chapter" data-level="13.3.4" data-path="report.html"><a href="report.html#shinydashboard"><i class="fa fa-check"></i><b>13.3.4</b> shinydashboard</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="report.html"><a href="report.html#flexdashboard"><i class="fa fa-check"></i><b>13.4</b> flexdashboard</a></li>
<li class="chapter" data-level="13.5" data-path="report.html"><a href="report.html#bibliographic-notes-11"><i class="fa fa-check"></i><b>13.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="13.6" data-path="report.html"><a href="report.html#practice-yourself-9"><i class="fa fa-check"></i><b>13.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="sparse.html"><a href="sparse.html"><i class="fa fa-check"></i><b>14</b> Sparse Representations</a><ul>
<li class="chapter" data-level="14.1" data-path="sparse.html"><a href="sparse.html#sparse-matrix-representations"><i class="fa fa-check"></i><b>14.1</b> Sparse Matrix Representations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="sparse.html"><a href="sparse.html#coo"><i class="fa fa-check"></i><b>14.1.1</b> Coordinate List Representation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sparse.html"><a href="sparse.html#compressed-row-oriented-representation"><i class="fa fa-check"></i><b>14.1.2</b> Compressed Row Oriented Representation</a></li>
<li class="chapter" data-level="14.1.3" data-path="sparse.html"><a href="sparse.html#compressed-column-oriented-representation"><i class="fa fa-check"></i><b>14.1.3</b> Compressed Column Oriented Representation</a></li>
<li class="chapter" data-level="14.1.4" data-path="sparse.html"><a href="sparse.html#sparse-algorithms"><i class="fa fa-check"></i><b>14.1.4</b> Sparse Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sparse.html"><a href="sparse.html#sparse-matrices-and-sparse-models-in-r"><i class="fa fa-check"></i><b>14.2</b> Sparse Matrices and Sparse Models in R</a><ul>
<li class="chapter" data-level="14.2.1" data-path="sparse.html"><a href="sparse.html#the-matrix-package"><i class="fa fa-check"></i><b>14.2.1</b> The Matrix Package</a></li>
<li class="chapter" data-level="14.2.2" data-path="sparse.html"><a href="sparse.html#the-glmnet-package"><i class="fa fa-check"></i><b>14.2.2</b> The glmnet Package</a></li>
<li class="chapter" data-level="14.2.3" data-path="sparse.html"><a href="sparse.html#the-matrixmodels-package"><i class="fa fa-check"></i><b>14.2.3</b> The MatrixModels Package</a></li>
<li class="chapter" data-level="14.2.4" data-path="sparse.html"><a href="sparse.html#the-sparsem-package"><i class="fa fa-check"></i><b>14.2.4</b> The SparseM Package</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="sparse.html"><a href="sparse.html#beyond-sparsity"><i class="fa fa-check"></i><b>14.3</b> Beyond Sparsity</a></li>
<li class="chapter" data-level="14.4" data-path="sparse.html"><a href="sparse.html#apache-arrow"><i class="fa fa-check"></i><b>14.4</b> Apache Arrow</a></li>
<li class="chapter" data-level="14.5" data-path="sparse.html"><a href="sparse.html#bibliographic-notes-12"><i class="fa fa-check"></i><b>14.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="14.6" data-path="sparse.html"><a href="sparse.html#practice-yourself-10"><i class="fa fa-check"></i><b>14.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="memory.html"><a href="memory.html"><i class="fa fa-check"></i><b>15</b> Memory Efficiency</a><ul>
<li class="chapter" data-level="15.1" data-path="memory.html"><a href="memory.html#efficient-computing-from-ram"><i class="fa fa-check"></i><b>15.1</b> Efficient Computing from RAM</a><ul>
<li class="chapter" data-level="15.1.1" data-path="memory.html"><a href="memory.html#summary-statistics-from-ram"><i class="fa fa-check"></i><b>15.1.1</b> Summary Statistics from RAM</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="memory.html"><a href="memory.html#computing-from-a-database"><i class="fa fa-check"></i><b>15.2</b> Computing from a Database</a></li>
<li class="chapter" data-level="15.3" data-path="memory.html"><a href="memory.html#file-structure"><i class="fa fa-check"></i><b>15.3</b> Computing From Efficient File Structrures</a><ul>
<li class="chapter" data-level="15.3.1" data-path="memory.html"><a href="memory.html#bigmemory"><i class="fa fa-check"></i><b>15.3.1</b> bigmemory</a></li>
<li class="chapter" data-level="15.3.2" data-path="memory.html"><a href="memory.html#bigstep"><i class="fa fa-check"></i><b>15.3.2</b> bigstep</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="memory.html"><a href="memory.html#ff"><i class="fa fa-check"></i><b>15.4</b> ff</a></li>
<li class="chapter" data-level="15.5" data-path="memory.html"><a href="memory.html#disk.frame"><i class="fa fa-check"></i><b>15.5</b> disk.frame</a></li>
<li class="chapter" data-level="15.6" data-path="memory.html"><a href="memory.html#matter"><i class="fa fa-check"></i><b>15.6</b> matter</a></li>
<li class="chapter" data-level="15.7" data-path="memory.html"><a href="memory.html#iotools"><i class="fa fa-check"></i><b>15.7</b> iotools</a></li>
<li class="chapter" data-level="15.8" data-path="memory.html"><a href="memory.html#hdf5"><i class="fa fa-check"></i><b>15.8</b> HDF5</a></li>
<li class="chapter" data-level="15.9" data-path="memory.html"><a href="memory.html#delayedarray"><i class="fa fa-check"></i><b>15.9</b> DelayedArray</a></li>
<li class="chapter" data-level="15.10" data-path="memory.html"><a href="memory.html#computing-from-a-distributed-file-system"><i class="fa fa-check"></i><b>15.10</b> Computing from a Distributed File System</a></li>
<li class="chapter" data-level="15.11" data-path="memory.html"><a href="memory.html#bibliographic-notes-13"><i class="fa fa-check"></i><b>15.11</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="15.12" data-path="memory.html"><a href="memory.html#practice-yourself-11"><i class="fa fa-check"></i><b>15.12</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="parallel.html"><a href="parallel.html"><i class="fa fa-check"></i><b>16</b> Parallel Computing</a><ul>
<li class="chapter" data-level="16.1" data-path="parallel.html"><a href="parallel.html#when-and-how-to-parallelise"><i class="fa fa-check"></i><b>16.1</b> When and How to Parallelise?</a></li>
<li class="chapter" data-level="16.2" data-path="parallel.html"><a href="parallel.html#terminology"><i class="fa fa-check"></i><b>16.2</b> Terminology</a><ul>
<li class="chapter" data-level="16.2.1" data-path="parallel.html"><a href="parallel.html#hardware"><i class="fa fa-check"></i><b>16.2.1</b> Hardware:</a></li>
<li class="chapter" data-level="16.2.2" data-path="parallel.html"><a href="parallel.html#software"><i class="fa fa-check"></i><b>16.2.2</b> Software:</a></li>
<li class="chapter" data-level="16.2.3" data-path="parallel.html"><a href="parallel.html#types-of-parallelism"><i class="fa fa-check"></i><b>16.2.3</b> Types of Parallelism</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="parallel.html"><a href="parallel.html#parallel-r"><i class="fa fa-check"></i><b>16.3</b> Parallel R</a><ul>
<li class="chapter" data-level="16.3.1" data-path="parallel.html"><a href="parallel.html#the-parallel-package"><i class="fa fa-check"></i><b>16.3.1</b> The parallel Package</a></li>
<li class="chapter" data-level="16.3.2" data-path="parallel.html"><a href="parallel.html#the-foreach-package"><i class="fa fa-check"></i><b>16.3.2</b> The foreach Package</a></li>
<li class="chapter" data-level="16.3.3" data-path="parallel.html"><a href="parallel.html#rdsm"><i class="fa fa-check"></i><b>16.3.3</b> Rdsm</a></li>
<li class="chapter" data-level="16.3.4" data-path="parallel.html"><a href="parallel.html#pbdr"><i class="fa fa-check"></i><b>16.3.4</b> pbdR</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="parallel.html"><a href="parallel.html#parallel-extensions"><i class="fa fa-check"></i><b>16.4</b> Parallel Extensions</a><ul>
<li class="chapter" data-level="16.4.1" data-path="parallel.html"><a href="parallel.html#parallel-linear-algebra"><i class="fa fa-check"></i><b>16.4.1</b> Parallel Linear Algebra</a></li>
<li class="chapter" data-level="16.4.2" data-path="parallel.html"><a href="parallel.html#parallel-data-munging-with-data.table"><i class="fa fa-check"></i><b>16.4.2</b> Parallel Data Munging with data.table</a></li>
<li class="chapter" data-level="16.4.3" data-path="parallel.html"><a href="parallel.html#spark"><i class="fa fa-check"></i><b>16.4.3</b> Spark</a></li>
<li class="chapter" data-level="16.4.4" data-path="parallel.html"><a href="parallel.html#h2o"><i class="fa fa-check"></i><b>16.4.4</b> H2O</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="parallel.html"><a href="parallel.html#nested-parallel"><i class="fa fa-check"></i><b>16.5</b> Caution: Nested Parallelism</a></li>
<li class="chapter" data-level="16.6" data-path="parallel.html"><a href="parallel.html#bibliographic-notes-14"><i class="fa fa-check"></i><b>16.6</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="16.7" data-path="parallel.html"><a href="parallel.html#practice-yourself-12"><i class="fa fa-check"></i><b>16.7</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="algebra.html"><a href="algebra.html"><i class="fa fa-check"></i><b>17</b> Numerical Linear Algebra</a><ul>
<li class="chapter" data-level="17.1" data-path="algebra.html"><a href="algebra.html#lu-factorization"><i class="fa fa-check"></i><b>17.1</b> LU Factorization</a></li>
<li class="chapter" data-level="17.2" data-path="algebra.html"><a href="algebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>17.2</b> Cholesky Factorization</a></li>
<li class="chapter" data-level="17.3" data-path="algebra.html"><a href="algebra.html#qr-factorization"><i class="fa fa-check"></i><b>17.3</b> QR Factorization</a></li>
<li class="chapter" data-level="17.4" data-path="algebra.html"><a href="algebra.html#singular-value-factorization"><i class="fa fa-check"></i><b>17.4</b> Singular Value Factorization</a></li>
<li class="chapter" data-level="17.5" data-path="algebra.html"><a href="algebra.html#iterative-methods"><i class="fa fa-check"></i><b>17.5</b> Iterative Methods</a></li>
<li class="chapter" data-level="17.6" data-path="algebra.html"><a href="algebra.html#solving-ols"><i class="fa fa-check"></i><b>17.6</b> Solving the OLS Problem</a></li>
<li class="chapter" data-level="17.7" data-path="algebra.html"><a href="algebra.html#numerical-libraries-for-linear-algebra"><i class="fa fa-check"></i><b>17.7</b> Numerical Libraries for Linear Algebra</a><ul>
<li class="chapter" data-level="17.7.1" data-path="algebra.html"><a href="algebra.html#openblas"><i class="fa fa-check"></i><b>17.7.1</b> OpenBlas</a></li>
<li class="chapter" data-level="17.7.2" data-path="algebra.html"><a href="algebra.html#mkl"><i class="fa fa-check"></i><b>17.7.2</b> MKL</a></li>
</ul></li>
<li class="chapter" data-level="17.8" data-path="algebra.html"><a href="algebra.html#bibliographic-notes-15"><i class="fa fa-check"></i><b>17.8</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="17.9" data-path="algebra.html"><a href="algebra.html#practice-yourself-13"><i class="fa fa-check"></i><b>17.9</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="convex.html"><a href="convex.html"><i class="fa fa-check"></i><b>18</b> Convex Optimization</a><ul>
<li class="chapter" data-level="18.1" data-path="convex.html"><a href="convex.html#theoretical-backround"><i class="fa fa-check"></i><b>18.1</b> Theoretical Backround</a></li>
<li class="chapter" data-level="18.2" data-path="convex.html"><a href="convex.html#optimizing-with-r"><i class="fa fa-check"></i><b>18.2</b> Optimizing with R</a><ul>
<li class="chapter" data-level="18.2.1" data-path="convex.html"><a href="convex.html#the-optim-function"><i class="fa fa-check"></i><b>18.2.1</b> The optim Function</a></li>
<li class="chapter" data-level="18.2.2" data-path="convex.html"><a href="convex.html#the-nloptr-package"><i class="fa fa-check"></i><b>18.2.2</b> The nloptr Package</a></li>
<li class="chapter" data-level="18.2.3" data-path="convex.html"><a href="convex.html#minqa-package"><i class="fa fa-check"></i><b>18.2.3</b> minqa Package</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="convex.html"><a href="convex.html#bibliographic-notes-16"><i class="fa fa-check"></i><b>18.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="18.4" data-path="convex.html"><a href="convex.html#practice-yourself-14"><i class="fa fa-check"></i><b>18.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="rcpp.html"><a href="rcpp.html"><i class="fa fa-check"></i><b>19</b> RCpp</a><ul>
<li class="chapter" data-level="19.1" data-path="rcpp.html"><a href="rcpp.html#bibliographic-notes-17"><i class="fa fa-check"></i><b>19.1</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="19.2" data-path="rcpp.html"><a href="rcpp.html#practice-yourself-15"><i class="fa fa-check"></i><b>19.2</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="debugging.html"><a href="debugging.html"><i class="fa fa-check"></i><b>20</b> Debugging Tools</a><ul>
<li class="chapter" data-level="20.1" data-path="debugging.html"><a href="debugging.html#bibliographic-notes-18"><i class="fa fa-check"></i><b>20.1</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="20.2" data-path="debugging.html"><a href="debugging.html#practice-yourself-16"><i class="fa fa-check"></i><b>20.2</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="hadley.html"><a href="hadley.html"><i class="fa fa-check"></i><b>21</b> The Hadleyverse</a><ul>
<li class="chapter" data-level="21.1" data-path="hadley.html"><a href="hadley.html#readr"><i class="fa fa-check"></i><b>21.1</b> readr</a></li>
<li class="chapter" data-level="21.2" data-path="hadley.html"><a href="hadley.html#dplyr"><i class="fa fa-check"></i><b>21.2</b> dplyr</a></li>
<li class="chapter" data-level="21.3" data-path="hadley.html"><a href="hadley.html#tidyr"><i class="fa fa-check"></i><b>21.3</b> tidyr</a></li>
<li class="chapter" data-level="21.4" data-path="hadley.html"><a href="hadley.html#reshape2"><i class="fa fa-check"></i><b>21.4</b> reshape2</a></li>
<li class="chapter" data-level="21.5" data-path="hadley.html"><a href="hadley.html#stringr"><i class="fa fa-check"></i><b>21.5</b> stringr</a></li>
<li class="chapter" data-level="21.6" data-path="hadley.html"><a href="hadley.html#anytime"><i class="fa fa-check"></i><b>21.6</b> anytime</a></li>
<li class="chapter" data-level="21.7" data-path="hadley.html"><a href="hadley.html#biblipgraphic-notes"><i class="fa fa-check"></i><b>21.7</b> Biblipgraphic Notes</a></li>
<li class="chapter" data-level="21.8" data-path="hadley.html"><a href="hadley.html#practice-yourself-17"><i class="fa fa-check"></i><b>21.8</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>22</b> Causal Inferense</a><ul>
<li class="chapter" data-level="22.1" data-path="causality.html"><a href="causality.html#causal-inference-from-designed-experiments"><i class="fa fa-check"></i><b>22.1</b> Causal Inference From Designed Experiments</a><ul>
<li class="chapter" data-level="22.1.1" data-path="causality.html"><a href="causality.html#design-of-experiments"><i class="fa fa-check"></i><b>22.1.1</b> Design of Experiments</a></li>
<li class="chapter" data-level="22.1.2" data-path="causality.html"><a href="causality.html#randomized-inference"><i class="fa fa-check"></i><b>22.1.2</b> Randomized Inference</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="causality.html"><a href="causality.html#causal-inference-from-observational-data"><i class="fa fa-check"></i><b>22.2</b> Causal Inference from Observational Data</a><ul>
<li class="chapter" data-level="22.2.1" data-path="causality.html"><a href="causality.html#principal-stratification"><i class="fa fa-check"></i><b>22.2.1</b> Principal Stratification</a></li>
<li class="chapter" data-level="22.2.2" data-path="causality.html"><a href="causality.html#instrumental-variables"><i class="fa fa-check"></i><b>22.2.2</b> Instrumental Variables</a></li>
<li class="chapter" data-level="22.2.3" data-path="causality.html"><a href="causality.html#propensity-scores"><i class="fa fa-check"></i><b>22.2.3</b> Propensity Scores</a></li>
<li class="chapter" data-level="22.2.4" data-path="causality.html"><a href="causality.html#direct-lieklihood"><i class="fa fa-check"></i><b>22.2.4</b> Direct Lieklihood</a></li>
<li class="chapter" data-level="22.2.5" data-path="causality.html"><a href="causality.html#regression-discontinuity"><i class="fa fa-check"></i><b>22.2.5</b> Regression Discontinuity</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="causality.html"><a href="causality.html#bibliographic-notes-19"><i class="fa fa-check"></i><b>22.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="22.4" data-path="causality.html"><a href="causality.html#practice-yourself-18"><i class="fa fa-check"></i><b>22.4</b> Practice Yourself</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R (BGU course)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parallel" class="section level1">
<h1><span class="header-section-number">Chapter 16</span> Parallel Computing</h1>
<p>You would think that because you have an expensive multicore computer your computations will speed up.
Well, no; unless you actively make sure of that.
By default, no matter how many cores you have, the operating system will allocate each R session to a single core.</p>
<p>You may wonder: why can’t I just write code, and let R (or any other language) figure out what can be parallelised.
Well, that’s not how things work.
It is very hard to design software that can parallelise any algorithm, while adapting to your hardware, operating system, and other the software running on your device.
A lot of parallelisation still has to be explicit, but stay tuned for technologies like <a href="https://rise.cs.berkeley.edu/projects/ray/">Ray</a>, <a href="https://spark.apache.org">Apache Spark</a>, <a href="https://flink.apache.org">Apache Flink</a>, <a href="https://chapel-lang.org">Chapel</a>, <a href="https://pytorch.org">PyTorch</a>, and others, which are making great advances in handling parallelism for you.</p>
<p>To parallelise computations, we need to distinguish between two types of parallelism:</p>
<ol style="list-style-type: decimal">
<li><strong>Parallel R</strong>: where the parallelism is managed with R. Discussed in Section <a href="parallel.html#parallel-r">16.3</a>.</li>
<li><strong>Parallel Extensions</strong>: where R calls specialized libraries/routines/software that manage the parallelism themselves. Discussed in Section <a href="parallel.html#parallel-extensions">16.4</a>.</li>
</ol>
<div id="when-and-how-to-parallelise" class="section level2">
<h2><span class="header-section-number">16.1</span> When and How to Parallelise?</h2>
<p>Your computations are too slow.
Why is this?
Should you store your data differently?
Should you use different software?
Should you buy more RAM?
Should you “go cloud”?</p>
<p>Unlike what some vendors will make you think, there is no one-size-fits-all solution to speed problems.
Solving a RAM bottleneck may consume more CPU.
Solving a CPU bottleneck may consume more RAM.
Parallelisation means using multiple CPUs simultaneously.
It will thus aid with CPU bottlenecks, but may consume more RAM.
Parallelising is thus ill advised when dealing with a RAM bottleneck.
Memory bottlenecks are released with sparsity (Chapter <a href="sparse.html#sparse">14</a>), or efficient memory usage (Chapter <a href="memory.html#memory">15</a>).</p>
<p>When deciding if, and how, to parallelise, it is crucial that you diagnose your bottleneck.
The good news is- that diagnostics is not too hard.
Here are a few pointers:</p>
<ol style="list-style-type: decimal">
<li><p>Always have a system monitor open. Windows users have their <a href="https://en.wikipedia.org/wiki/Task_Manager_(Windows)">Task Manager</a>; Linux users have <a href="https://en.wikipedia.org/wiki/Top_(software)">top</a>, or preferably, <a href="https://en.wikipedia.org/wiki/Htop">htop</a>; Mac users have the <a href="https://www.howtogeek.com/227240/how-to-monitor-your-macs-health-with-activity-monitor/">Activity Monitor</a>. The system monitor will inform you how many CPUs are being used, and how much RAM is being used.</p></li>
<li><p>If you escape your computation, and R takes a long time to respond, you are probably dealing with a RAM bottleneck.</p></li>
<li><p>Profile your code to detect how much RAM and CPU are consumed by each line of code. See Hadley’s <a href="http://adv-r.had.co.nz/Profiling.html">guide</a>.</p></li>
</ol>
<p>In the best possible scenario, the number of operations you can perform scales with the number of processors: <span class="math display">\[time * processors = operations\]</span>.
This is called <em>perfect scaling</em>.
It is rarely observed in practice, since parallelising incurs some computational overhead: setting up environments, copying memory, …
For this reason, the typical speedup is sub-linear.
Computer scientists call this <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s law</a>.</p>
</div>
<div id="terminology" class="section level2">
<h2><span class="header-section-number">16.2</span> Terminology</h2>
<p>Here are some terms we will be needing.</p>
<div id="hardware" class="section level3">
<h3><span class="header-section-number">16.2.1</span> Hardware:</h3>
<ul>
<li><strong>Cluster:</strong> A collection of interconnected computers.</li>
<li><strong>Node/Machine:</strong> A single physical machine in the cluster. Components of a single node do not communicate via the cluster’s network, but rather, via the node’s circuitry.</li>
<li><strong>Processor/Socket/CPU:</strong></li>
<li><strong>RAM:</strong> Random Access Memory. One of many types of memory in a computer. Possibly the most relevant type of memory when computing with data.</li>
<li><strong>GPU:</strong> Graphical Processing Unit. A computing unit, separate from the CPU. Originally dedicated to graphics and gaming, thus its name. Currently, GPUs are extremely popular for fitting and servicing Deep Neural Networks.</li>
<li><strong>TPU:</strong> Tensor Processing Unit. A computing unit, dedicated to servicing and fitting Deep Neural Networks.</li>
</ul>
</div>
<div id="software" class="section level3">
<h3><span class="header-section-number">16.2.2</span> Software:</h3>
<ul>
<li><strong>Process:</strong> A sequence of instructions in memory, with accompanying data. Various processes typically see different locations of memory. Interpreted languages like R, and Python operate on processes.</li>
<li><strong>Thread:</strong> A sub-sequence of instructions, within a process. Various threads in a process may see the same memory. Compiled languages like C, C++, may operate on threads.</li>
</ul>
</div>
<div id="types-of-parallelism" class="section level3">
<h3><span class="header-section-number">16.2.3</span> Types of Parallelism</h3>
<ul>
<li><strong>Data Parallel:</strong> When performing different tasks on the same data.</li>
<li><strong>Task Parallel:</strong> When performing the same task, on different data.</li>
</ul>
</div>
</div>
<div id="parallel-r" class="section level2">
<h2><span class="header-section-number">16.3</span> Parallel R</h2>
<p>R provides many frameworks for explicit parallelism.
Any such framework will include the means for starting R (salve/worker) sessions, and the means for communicating between these sessions.
Except for developers, a typical user will probably be using some high-level R package which will abstract away these stages.
A communication protocol because different R sessions will be communication one with the other. An abstraction layer because providing the user will all the features and generality of the communication layer is needlessly cumbersome.</p>
<p>Starting new R processes, a.k.a. slaves:</p>
<ul>
<li><p>R sessions/process may always be started using system calls. This creates <em>child processes</em>, or <em>spawn processes</em> that cannot access the data and instructions of the calling process.</p></li>
<li><p><strong>Fork</strong>: a mechanism, unique to Unix and Linux, that clones a process with accompanying instructions and data. All processes see the same memory in read-only state. Copies are made when data is changed by a process.</p></li>
</ul>
<p>Inter-process communication standards:</p>
<ul>
<li><p><strong>Socket</strong>: data sent via a network interface. Put differently- information is sent between R processes as if these were different machines in a network. Information may be structured by the particular application, and does not abide to some</p></li>
<li><p><strong>Parallel Virtual Machine</strong> (PVM): a communication protocol and software, developed the University of Tennessee, Oak Ridge National Laboratory and Emory University, and first released in 1989. Runs on Windows and Unix, thus allowing to compute on clusters running these two operating systems. Noways, it is mostly replaced by MPI. The same group responsible for PVM will later deliver <em>Programming with Big Data in R</em> (<a href="https://pbdr.org">pbdR</a>): a whole ecosystem of packages for running R on large computing clusters.</p></li>
<li><p><strong>Message Passing Interface</strong> (MPI): A communication protocol that has become the de-facto standard for communication in large distributed clusters. Particularly, for heterogeneous computing clusters with varying operating systems and hardware. The protocol has various software implementations such as <a href="https://en.wikipedia.org/wiki/Open_MPI">OpenMPI</a> and <a href="https://en.wikipedia.org/wiki/MPICH">MPICH</a>, <a href="http://mpi.deino.net/">Deino</a>, <a href="https://en.wikipedia.org/wiki/LAM/MPI">LAM/MPI</a>. Interestingly, large computing clusters use MPI, while modern BigData analysis platforms such as Spark, and Ray do not. Why is this? See Jonathan Dursi’s excellent but controversial <a href="https://www.dursi.ca/post/hpc-is-dying-and-mpi-is-killing-it.html">blog post</a>.</p></li>
<li><p><strong>NetWorkSpaces</strong> (NWS): A master-slave communication protocol where the master is not an R-session, but rather, an <em>NWS server</em>.</p></li>
</ul>
<p>For more on inter-process communication, see <a href="https://en.wikipedia.org/wiki/Inter-process_communication">Wiki</a>.</p>
<div id="the-parallel-package" class="section level3">
<h3><span class="header-section-number">16.3.1</span> The parallel Package</h3>
<p>The <strong>parallel</strong> package, maintained by the R-core team, was introduced in 2011 to unify two popular parallisation packages: <strong>snow</strong> and <strong>multicore</strong>.
The <strong>multicore</strong> package was designed to parallelise using the <em>fork</em> mechanism, on Linux machines.
The <strong>snow</strong> package was designed to parallelise using the <em>spawn</em> mechanism, on all operating systems.
Servers/R-sessions started with <strong>snow</strong> will not see the parent’s data, which will have to be copied to spawned sessions.
At least there is less data redundancy.
Spawning, unlike forking, can be done on remote machines, which communicate using MPI.</p>
<p>TOOD: add example.</p>
</div>
<div id="the-foreach-package" class="section level3">
<h3><span class="header-section-number">16.3.2</span> The foreach Package</h3>
<p>For reasons detailed in <span class="citation">Kane et al. (<a href="#ref-kane2013scalable">2013</a>)</span>, we recommend the <strong>foreach</strong> parallelisation package <span class="citation">(Analytics and Weston <a href="#ref-foreach">2015</a>)</span>.
It allows us to:</p>
<ol style="list-style-type: decimal">
<li><p>Decouple between our parallel algorithm and the parallelisation mechanism: we write parallelisable code once, and can later switch between parallelisation mechanisms.
Currently supported mechanisms include:</p>
<ul>
<li><em>fork</em>: Called with the <em>doMC</em> backend.</li>
<li><em>MPI</em>, <em>VPM</em>, <em>NWS</em>: Called with the <em>doSNOW</em> or <em>doMPI</em> backends.</li>
<li><em>futures</em>: Called with the <em>doFuture</em> backend.</li>
<li><em>redis</em>: Called with the <em>doRedis</em> backend. Similar to NWS, only that data made available to different processes using <a href="https://en.wikipedia.org/wiki/Redis">Redis</a>.</li>
<li>Future mechanism may also be supported.</li>
</ul></li>
<li><p>Combine with the <code>big.matrix</code> object from Chapter <a href="memory.html#memory">15</a> for <em>shared memory parallelisation</em>: all the machines may see the same data, so that we don’t need to export objects from machine to machine.</p></li>
</ol>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> I personally prefer the <strong>multicore</strong> mechanism, with the <strong>doMC</strong> adapter for <strong>foreach</strong>.
I will not use this combo, however, because <strong>multicore</strong> will not work on Windows machines, and will not work over a network.
I will thus use the more general <strong>snow</strong> and <strong>doParallel</strong> combo.
If you do happen to run on Linux, or Unix, you will want to replace all <strong>doParallel</strong> functionality with <strong>doMC</strong>.
</div>

<p>Let’s start with a simple example, taken from <a href="http://debian.mc.vanderbilt.edu/R/CRAN/web/packages/doParallel/vignettes/gettingstartedParallel.pdf">“Getting Started with doParallel and foreach”</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(doParallel)</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">cl &lt;-<span class="st"> </span><span class="kw">makeCluster</span>(<span class="dv">2</span>, <span class="dt">type =</span> <span class="st">&#39;SOCK&#39;</span>)
<span class="kw">registerDoParallel</span>(cl)
result &lt;-<span class="st"> </span><span class="kw">foreach</span>(<span class="dt">i=</span><span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) <span class="op">%dopar%</span><span class="st"> </span><span class="kw">sqrt</span>(i)
<span class="kw">class</span>(result)</code></pre>
<pre><code>## [1] &quot;list&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">result</code></pre>
<pre><code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051</code></pre>
<p>Things to note:</p>
<ul>
<li><code>makeCluster</code> creates an object with the information our cluster.
On a single machine it is very simple. On a cluster of machines, you will need to specify the i.p. addresses or other identifiers of the machines.</li>
<li><code>registerDoParallel</code> is used to inform the <strong>foreach</strong> package of the presence of our cluster.</li>
<li>The <code>foreach</code> function handles the looping. In particular note the <code>%dopar</code> operator that ensures that looping is in parallel. <code>%dopar%</code> can be replaced by <code>%do%</code> if you want serial looping (like the <code>for</code> loop), for instance, for debugging.</li>
<li>The output of the various machines is collected by <code>foreach</code> to a list object.</li>
<li>In this simple example, no data is shared between machines so we are not putting the shared memory capabilities to the test.</li>
<li>We can check how many workers were involved using the <code>getDoParWorkers()</code> function.</li>
<li>We can check the parallelisation mechanism used with the <code>getDoParName()</code> function.</li>
</ul>
<p>Here is a more involved example.
We now try to make <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrap</a> inference on the coefficients of a logistic regression.
Bootstrapping means that in each iteration, we resample the data, and refit the model.</p>
<pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>iris[<span class="kw">which</span>(iris[,<span class="dv">5</span>] <span class="op">!=</span><span class="st"> &quot;setosa&quot;</span>), <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>)]
trials &lt;-<span class="st"> </span><span class="fl">1e4</span>
ptime &lt;-<span class="st"> </span><span class="kw">system.time</span>({
 r &lt;-<span class="st"> </span><span class="kw">foreach</span>(<span class="kw">icount</span>(trials), <span class="dt">.combine=</span>cbind) <span class="op">%dopar%</span><span class="st"> </span>{
 ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">100</span>, <span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)
 result1 &lt;-<span class="st"> </span><span class="kw">glm</span>(x[ind,<span class="dv">2</span>]<span class="op">~</span>x[ind,<span class="dv">1</span>], <span class="dt">family=</span><span class="kw">binomial</span>(logit))
 <span class="kw">coefficients</span>(result1)
 }
 })[<span class="dv">3</span>]
ptime</code></pre>
<pre><code>## elapsed 
##  11.521</code></pre>
<p>Things to note:</p>
<ul>
<li>As usual, we use the <code>foreach</code> function with the <code>%dopar%</code> operator to loop in parallel.</li>
<li>The <code>icounts</code> function generates a counter.</li>
<li>The <code>.combine=cbind</code> argument tells the <code>foreach</code> function how to combine the output of different machines, so that the returned object is not the default list.</li>
</ul>
<p>How long would that have taken in a simple (serial) loop?
We only need to replace <code>%dopar%</code> with <code>%do%</code> to test.</p>
<pre class="sourceCode r"><code class="sourceCode r">stime &lt;-<span class="st"> </span><span class="kw">system.time</span>({
 r &lt;-<span class="st"> </span><span class="kw">foreach</span>(<span class="kw">icount</span>(trials), <span class="dt">.combine=</span>cbind) <span class="op">%do%</span><span class="st"> </span>{
 ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">100</span>, <span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)
 result1 &lt;-<span class="st"> </span><span class="kw">glm</span>(x[ind,<span class="dv">2</span>]<span class="op">~</span>x[ind,<span class="dv">1</span>], <span class="dt">family=</span><span class="kw">binomial</span>(logit))
 <span class="kw">coefficients</span>(result1)
 }
 })[<span class="dv">3</span>]
stime</code></pre>
<pre><code>## elapsed 
##  19.217</code></pre>
<p>Yes. Parallelising is clearly faster.</p>
<p>Let’s see how we can combine the power of <strong>bigmemory</strong> and <strong>foreach</strong> by creating a file mapped <code>big.matrix</code> object, which is shared by all machines.
The following example is taken from <span class="citation">Kane et al. (<a href="#ref-kane2013scalable">2013</a>)</span>, and uses the <code>big.matrix</code> object we created in Chapter <a href="memory.html#memory">15</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(bigmemory)
x &lt;-<span class="st"> </span><span class="kw">attach.big.matrix</span>(<span class="st">&quot;airline.desc&quot;</span>)

<span class="kw">library</span>(foreach)
<span class="kw">library</span>(doSNOW)</code></pre>
<pre><code>## Loading required package: snow</code></pre>
<pre><code>## 
## Attaching package: &#39;snow&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:parallel&#39;:
## 
##     clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,
##     clusterExport, clusterMap, clusterSplit, makeCluster,
##     parApply, parCapply, parLapply, parRapply, parSapply,
##     splitIndices, stopCluster</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">cl &lt;-<span class="st"> </span><span class="kw">makeSOCKcluster</span>(<span class="kw">rep</span>(<span class="st">&quot;localhost&quot;</span>, <span class="dv">4</span>)) <span class="co"># make a cluster of 4 machines</span>
<span class="kw">registerDoSNOW</span>(cl) <span class="co"># register machines for foreach()</span></code></pre>
<p>Get a “description” of the <code>big.matrix</code> object that will be used to call it from each machine.</p>
<pre class="sourceCode r"><code class="sourceCode r">xdesc &lt;-<span class="st"> </span><span class="kw">describe</span>(x) </code></pre>
<p>Split the data along values of <code>BENE_AGE_CAT_CD</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">G &lt;-<span class="st"> </span><span class="kw">split</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(x), x[, <span class="st">&quot;BENE_AGE_CAT_CD&quot;</span>]) </code></pre>
<p>Define a function that computes quantiles of <code>CAR_LINE_ICD9_DGNS_CD</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">GetDepQuantiles &lt;-<span class="st"> </span><span class="cf">function</span>(rows, data) {
 <span class="kw">quantile</span>(data[rows, <span class="st">&quot;CAR_LINE_ICD9_DGNS_CD&quot;</span>], <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="fl">0.99</span>),
 <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)
}</code></pre>
<p>We are all set up to loop, in parallel, and compute quantiles of <code>CAR_LINE_ICD9_DGNS_CD</code> for each value of <code>BENE_AGE_CAT_CD</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">qs &lt;-<span class="st"> </span><span class="kw">foreach</span>(<span class="dt">g =</span> G, <span class="dt">.combine =</span> rbind) <span class="op">%dopar%</span><span class="st"> </span>{
 <span class="kw">require</span>(<span class="st">&quot;bigmemory&quot;</span>)
 x &lt;-<span class="st"> </span><span class="kw">attach.big.matrix</span>(xdesc)
 <span class="kw">GetDepQuantiles</span>(<span class="dt">rows =</span> g, <span class="dt">data =</span> x)
}
qs</code></pre>
<pre><code>##          50% 90% 99%
## result.1 558 793 996
## result.2 518 789 996
## result.3 514 789 996
## result.4 511 789 996
## result.5 511 790 996
## result.6 518 796 995</code></pre>
<div id="which-backend-to-use" class="section level4">
<h4><span class="header-section-number">16.3.2.1</span> Which Backend to Use?</h4>
<p>In the following example, we compare the <em>fork</em> mechanism, to the <em>socket</em> mechanism.
The bottom line is- use <em>fork</em> whenever possible.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nycflights13)
flights<span class="op">$</span>ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">size =</span> <span class="kw">nrow</span>(flights), <span class="dt">replace =</span> <span class="ot">TRUE</span>)

timer &lt;-<span class="st"> </span><span class="cf">function</span>(i) <span class="kw">max</span>(flights[flights<span class="op">$</span>ind<span class="op">==</span>i,<span class="st">&quot;distance&quot;</span>])
<span class="kw">timer</span>(<span class="dv">1</span>)</code></pre>
<pre><code>## [1] 4983</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(doMC)
<span class="kw">registerDoMC</span>(<span class="dt">cores =</span> <span class="dv">10</span>)
<span class="kw">system.time</span>(<span class="kw">foreach</span> (<span class="dt">i=</span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">.combine =</span> <span class="st">&#39;c&#39;</span>) <span class="op">%dopar%</span><span class="st"> </span><span class="kw">timer</span>(i))</code></pre>
<pre><code>##    user  system elapsed 
##   0.018   0.480   0.504</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(parallel)         
<span class="kw">library</span>(doParallel)
cl &lt;-<span class="st"> </span><span class="kw">makeCluster</span>(<span class="dv">10</span>, <span class="dt">type=</span><span class="st">&quot;SOCK&quot;</span>)
<span class="kw">registerDoParallel</span>(cl)
<span class="kw">system.time</span>(<span class="kw">foreach</span> (<span class="dt">i=</span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">.combine =</span> <span class="st">&#39;c&#39;</span>) <span class="op">%dopar%</span><span class="st"> </span><span class="kw">timer</span>(i))</code></pre>
<pre><code>##    user  system elapsed 
##   1.112   0.146   2.071</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stopCluster</span>(cl)</code></pre>
<p>Things to note:</p>
<ul>
<li>The clear victor is <em>fork</em>: sessions start faster, and computations finish faster. Sadly, we recall that <em>forking</em> is impossible on Windows machines, or in clusters that consist of several machines.</li>
<li>We did not need to pass <code>flights</code> to the different workers. <code>foreach</code> took care of that for us.</li>
</ul>
<p>For fun, let’s try the same with <code>data.table</code>….</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(data.table)
flights.DT &lt;-<span class="st"> </span><span class="kw">as.data.table</span>(flights)
<span class="kw">system.time</span>(flights.DT[,<span class="kw">max</span>(distance),ind])</code></pre>
<pre><code>##    user  system elapsed 
##   0.064   0.098   0.118</code></pre>
<p>No surprises there.
If you can store your data in RAM, <code>data.table</code> is still the fastest.</p>
</div>
</div>
<div id="rdsm" class="section level3">
<h3><span class="header-section-number">16.3.3</span> Rdsm</h3>
<p>TODO</p>
</div>
<div id="pbdr" class="section level3">
<h3><span class="header-section-number">16.3.4</span> pbdR</h3>
<p>TODO</p>
</div>
</div>
<div id="parallel-extensions" class="section level2">
<h2><span class="header-section-number">16.4</span> Parallel Extensions</h2>
<p>As we have seen, R can be used to write explicit parallel algorithms.
Some algorithms, however, are so basic that others have already written and published their parallel versions.
We call these <em>parallel extensions</em>.
Linear algebra, and various machine learning algorithms are examples we now discuss.</p>
<div id="parallel-linear-algebra" class="section level3">
<h3><span class="header-section-number">16.4.1</span> Parallel Linear Algebra</h3>
<p>R ships with its own linear algebra algorithms, known as Basic Linear Algebra Subprograms: <a href="http://www.netlib.org/blas/">BLAS</a>.
To learn the history of linear algebra in R, read <span class="citation">Maechler and Bates (<a href="#ref-maechler20062nd">2006</a>)</span>.
For more details, see our Bibliographic notes.
As a user, you may actually replace the underlying linear algebra libraries in R.
There are actually many linear algebra libraries <a href="https://en.wikipedia.org/wiki/Comparison_of_linear_algebra_libraries">out there</a>.
Cutting edge linear algebra libraries such as <a href="https://github.com/xianyi/OpenBLAS">OpenBLAS</a>, <a href="https://bitbucket.org/icl/plasma/src/default/">Plasma</a>, and Intel’s <a href="https://software.intel.com/en-us/mkl">MKL</a> for Intel processors (which you are probably using), will parallelise basic linear algebra operations for you.
This is very useful, since: (a) all machines today have multiple cores and (b) linear algebra is at the heart of all statistics and machine learning.</p>
<p>Installing these libraries requires some knowldge in system administration.
It is quite simple under Ubuntu and Debian linux, and may be more comlicated otherwise.
Installing these is outside the scope of this text.
We will thus content ourselves with the following pointers:</p>
<ul>
<li>Users can easily replace the BLAS libraries shipped with R, with other libraries. OpenBLAS, and MKL. These libraries will parallelise linear algebra for you.
-This is easier to do under Ubuntu and Debian Linux, but possible in all systems.</li>
<li>For specific tasks, such as machine learning, you may not need an all-pupose paralle linear algebra library. If you want machine learning in parallel, there are more specialized libraries. In the followig, we demonstrate Spark (<a href="parallel.html#spark">16.4.3</a>), and H2O (@(h2o)).</li>
</ul>
</div>
<div id="parallel-data-munging-with-data.table" class="section level3">
<h3><span class="header-section-number">16.4.2</span> Parallel Data Munging with data.table</h3>
<p>We have discussed <code>data.table</code> on various occasions.
We now recall it to emphasize that various operations in <code>data.table</code> are done in parallel.
For instance, imports.
First, we check how many threads <code>data.table</code> is setup to use?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(data.table)
<span class="kw">getDTthreads</span>(<span class="dt">verbose=</span><span class="ot">TRUE</span>) </code></pre>
<pre><code>## omp_get_num_procs()==8
## R_DATATABLE_NUM_PROCS_PERCENT==&quot;&quot; (default 50)
## R_DATATABLE_NUM_THREADS==&quot;&quot;
## omp_get_thread_limit()==2147483647
## omp_get_max_threads()==8
## OMP_THREAD_LIMIT==&quot;&quot;
## OMP_NUM_THREADS==&quot;&quot;
## data.table is using 1 threads. This is set on startup, and by setDTthreads(). See ?setDTthreads.
## RestoreAfterFork==true</code></pre>
<pre><code>## [1] 1</code></pre>
<p>We then do the import and inspect CPU usage with the <em>top</em> linux command.</p>
<pre class="sourceCode r"><code class="sourceCode r">air &lt;-<span class="st"> </span><span class="kw">fread</span>(<span class="st">&#39;data/2010_BSA_Carrier_PUF.csv&#39;</span>)</code></pre>
<div class="figure">
<img src="art/Screenshot%20from%202019-10-03%2010-48-06.png" alt="CPU usage of fread(). The CPU usage is 301.7%. This is because data.table is setup to use 4 threads simultanously." />
<p class="caption">CPU usage of fread(). The CPU usage is 301.7%. This is because <code>data.table</code> is setup to use 4 threads simultanously.</p>
</div>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> An amazing feature of <code>data.table</code> is that it will not parallelize when called from a forked process.
This behaviour will avoid the nested parallelism we cautioned from in <a href="parallel.html#nested-parallel">16.5</a>.
</div>

<p>Now let’s compare parallelised aggregations:</p>
<pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st">  </span><span class="fl">5e6</span>
N &lt;-<span class="st"> </span>n
k &lt;-<span class="st">  </span><span class="fl">1e4</span>

<span class="kw">setDTthreads</span>(<span class="dt">threads =</span> <span class="dv">0</span>) <span class="co"># use all available cores</span>
<span class="kw">getDTthreads</span>() <span class="co"># print available threads</span></code></pre>
<pre><code>## [1] 8</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">DT &lt;-<span class="st">  </span><span class="kw">data.table</span>(<span class="dt">x =</span> <span class="kw">rep_len</span>(<span class="kw">runif</span>(n), N),
                <span class="dt">y =</span> <span class="kw">rep_len</span>(<span class="kw">runif</span>(n), N),
                <span class="dt">grp =</span> <span class="kw">rep_len</span>(<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>k, n, <span class="ot">TRUE</span>), N))

<span class="kw">system.time</span>(DT[, .(<span class="dt">a =</span> 1L), <span class="dt">by =</span> <span class="st">&quot;grp&quot;</span>])</code></pre>
<pre><code>##    user  system elapsed 
##   0.451   0.006   0.109</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">setDTthreads</span>(<span class="dt">threads =</span> <span class="dv">1</span>) <span class="co"># use a single thread</span>

<span class="kw">system.time</span>(DT[, .(<span class="dt">a =</span> 1L), <span class="dt">by =</span> <span class="st">&quot;grp&quot;</span>])</code></pre>
<pre><code>##    user  system elapsed 
##   0.181   0.000   0.181</code></pre>
<p>Things to note:</p>
<ul>
<li>Parallel aggregation is indeed much faster.</li>
<li>This example was cooked to emphasize the difference. You may not enjoy such speedups in all problems.</li>
</ul>
</div>
<div id="spark" class="section level3">
<h3><span class="header-section-number">16.4.3</span> Spark</h3>
<p>Spark is the brainchild of Matei Zaharia, in 2009, as part of his PhD studies at University of California, Berkeley ’s AMPLab.
To understand <em>Spark</em> need some background.
The software that manages files on your disk is the <a href="https://en.wikipedia.org/wiki/File_system">file system</a>.
On personal computers, you may have seen names like FAT32, NTFS, ext3, or others.
Those are disk file systems.
If your data is too big to be stored on a single disk, you may distribute it on several machines.
When doing so, you will need a file systems that is designed for distributed clusters.
A good <a href="https://en.wikipedia.org/wiki/Clustered_file_system">cluster file system</a>, is crucial for the performance of your cluster.
Google’s file system, the <a href="https://en.wikipedia.org/wiki/File_system">Google File</a>, is part of Google power.
If you are not at Google, you will not have access to this file system.
Luckily, there are many other alternatives.
The Hadoop File System, <a href="https://en.wikipedia.org/wiki/Apache_Hadoop">HDFS</a>, that started at Yahoo, and was ported to the Apache Foundation, is such an alternative.
With the HDFS you can store files in a cluster, but for doing statistics with the data in these files, you need software that is compatible with the file system.
Hadoop is such a software, but it was not designed for machine learning.
Hadoop operates from the disk.
Learning, which consists of a lot iterative algorithms run, requires fast and repeated data reads.
This is the purpose of Spark.
Spark is a data oriented computing environment over distributed file systems.
While originality linked to HDFS, it may run over many other distributed file systems, but also locally, as we now demonstrate.</p>
<pre><code>## Spark 2.4.0 for Hadoop 2.7 or later already installed.</code></pre>
<pre><code>## * Using Spark: 2.4.0</code></pre>
<p>Things to note:</p>
<ul>
<li><code>spark_install</code> will download and install <em>Spark</em> on your first run. Make sure to update the version number, as this text may be outdated by the time you read it.</li>
<li>I used the <em>sparklyr</em> package from RStudio. There is an alternative package from Apache: <em>SparkR</em>.</li>
<li><code>spark_connect</code> opens a connection to the Spark server. When working in a cluster, with many machines, the <code>master=</code> argumnt infrorms R which machine is the master, a.k.a. the “driver node”.</li>
<li>After running <code>spark_connect</code>, the connection to the Sprak server will appear in RStudio’s <a href="https://support.rstudio.com/hc/en-us/articles/115010915687-Using-RStudio-Connections">Connection pane</a>.</li>
</ul>
<p>Let’s load and aggregate some data:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nycflights13)
flights_tbl&lt;-<span class="st"> </span><span class="kw">copy_to</span>(<span class="dt">dest=</span>sc, <span class="dt">df=</span>flights, <span class="dt">name=</span><span class="st">&#39;flights&#39;</span>, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)
<span class="kw">class</span>(flights_tbl)</code></pre>
<pre><code>## [1] &quot;tbl_spark&quot; &quot;tbl_sql&quot;   &quot;tbl_lazy&quot;  &quot;tbl&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr) 
<span class="kw">system.time</span>(delay&lt;-flights_tbl <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">              </span><span class="kw">group_by</span>(tailnum) <span class="op">%&gt;%</span>
<span class="st">              </span><span class="kw">summarise</span>(
                <span class="dt">count=</span><span class="kw">n</span>(),<span class="dt">dist=</span><span class="kw">mean</span>(distance),<span class="dt">delay=</span><span class="kw">mean</span>(arr_delay)) <span class="op">%&gt;%</span>
<span class="st">              </span><span class="kw">filter</span>(count<span class="op">&gt;</span><span class="dv">20</span>, dist<span class="op">&lt;</span><span class="dv">2000</span>, <span class="op">!</span><span class="kw">is.na</span>(delay)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">              </span><span class="kw">collect</span>())</code></pre>
<pre><code>##    user  system elapsed 
##   0.287   2.772   3.467</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">delay</code></pre>
<pre><code>## # A tibble: 2,961 x 4
##    tailnum count  dist  delay
##    &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1 N24211    130 1330.  7.7  
##  2 N793JB    283 1529.  4.72 
##  3 N657JB    285 1286.  5.03 
##  4 N633AA     24 1587. -0.625
##  5 N9EAMQ    248  675.  9.24 
##  6 N3GKAA     77 1247.  4.97 
##  7 N997DL     63  868.  4.90 
##  8 N318NB    202  814. -1.12 
##  9 N651JB    261 1408.  7.58 
## 10 N841UA     96 1208.  2.10 
## # … with 2,951 more rows</code></pre>
<p>Things to note:</p>
<ul>
<li><code>copy_to</code> exports from R to Sprak. Typically, my data will already be waiting in Sprak, since the whole motivation is that it does not fit on y disk.</li>
<li>Notice the <code>collect</code> command at the end. As the name suggests, this will collect results from the various worker/slave machines.</li>
<li>I have used the <em>dplyr</em> syntax and not my favorite <em>data.table</em> syntax. This is because <em>sparklyr</em> currently supports the <em>splyr</em> syntax, or plain SQL with the <em>DBI</em> package.</li>
</ul>
<p>To make the most of it, you will porbably be running Spark on some cluster.
You should thus consult your cluster’s documentation in order to connect to it.
In our particular case, the data is not very big so it fits into RAM.
We can thus compare performance to <code>data.table</code>, only to re-discover, than if data fits in RAM, there is no beating <code>data.table</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(data.table)
flight.DT &lt;-<span class="st"> </span><span class="kw">data.table</span>(flights)
<span class="kw">system.time</span>(flight.DT[,.(<span class="dt">distance=</span><span class="kw">mean</span>(distance),<span class="dt">delay=</span><span class="kw">mean</span>(arr_delay),<span class="dt">count=</span>.N),<span class="dt">by=</span>tailnum][count<span class="op">&gt;</span><span class="dv">20</span> <span class="op">&amp;</span><span class="st"> </span>distance<span class="op">&lt;</span><span class="dv">2000</span> <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(delay)])</code></pre>
<pre><code>##    user  system elapsed 
##   0.039   0.069   0.106</code></pre>
<p>Let’s disconnect from the Spark server.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_disconnect</span>(sc)</code></pre>
<pre><code>## NULL</code></pre>
<p>Spark comes with a set of learning algorithms called <em>MLLib</em>.
Consult the <a href="http://spark.apache.org/docs/latest/ml-classification-regression.html">online documentation</a> to see which are currently available.
If your data is happily stored in a distributed Spark cluster, and the algorithm you want to run is not available, you have too options:
(1) use extensions or (2) write your own.</p>
<p>Writing your own algorithm and dispatching it to Spark can be done a-la <code>apply</code> style with <code>sparklyr::spark_apply</code>. This, however, would typically be extremely inneficient. You are better off finding a Spark extension that does what you need.
See the <em>sparklyr</em> <a href="https://CRAN.R-project.org/package=sparklyr">CRAN page</a>, and in particular the Reverse Depends section, to see which extensions are available.
One particular extension is <a href="https://cran.r-project.org/web/packages/rsparkling/index.html">rsparkling</a>, which allows you to apply H2O’s massive library of learning algorithms, on data stored in Spark.
We will not cover <em>sparklyr</em> herein, but we will present (standalone) H2O.</p>
</div>
<div id="h2o" class="section level3">
<h3><span class="header-section-number">16.4.4</span> H2O</h3>
<p>H2O can be thought of as a library of efficient distributed learning algorithm, that run in-memory, where memory considerations and parallelisation have been taken care of for you.
For a (massive) list of learning algorithms implemented in H2O, see <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html#">their documentaion</a>.
H2O can run as a standalone server, or on top of Spark, so that it may use the <em>Spark data frames</em>. This is the purpose of <em>Sparkling Water</em> discussed in <a href="parallel.html#sparkling">16.4.4.1</a>.
We start with working on H2O directly with the <code>h2o</code> package.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#install.packages(&quot;h2o&quot;) </span>
<span class="kw">suppressPackageStartupMessages</span>(<span class="kw">library</span>(h2o))

<span class="kw">h2o.init</span>(<span class="dt">nthreads=</span><span class="dv">2</span>) </code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         24 minutes 8 seconds 
##     H2O cluster timezone:       Etc/UTC 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.26.0.2 
##     H2O cluster version age:    2 months and 9 days  
##     H2O cluster name:           H2O_started_from_R_rstudio_gap243 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.42 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  2 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.6.1 (2019-07-05)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">h2o.no_progress</span>()
<span class="kw">data</span>(<span class="st">&quot;spam&quot;</span>, <span class="dt">package =</span> <span class="st">&#39;ElemStatLearn&#39;</span>)
spam.h2o &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(spam, <span class="dt">destination_frame =</span> <span class="st">&quot;spam.hex&quot;</span>) <span class="co"># load to the H2O server</span>
<span class="kw">h2o.ls</span>() <span class="co"># check avaialbe data in the server</span></code></pre>
<pre><code>##                                                                               key
## 1 modelmetrics_our.rf@3820919188453068362_on_RTMP_sid_b4f9_4@-8879209604009432720
## 2                                                                          our.rf
## 3                                      predictions_8635_our.rf_on_RTMP_sid_b4f9_6
## 4                                                                        spam.hex</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">h2o.describe</span>(spam.h2o) <span class="op">%&gt;%</span><span class="st"> </span>head <span class="co"># the H2O version of summary()</span></code></pre>
<pre><code>##   Label Type Missing Zeros PosInf NegInf Min   Max       Mean     Sigma
## 1   A.1 real       0  3548      0      0   0  4.54 0.10455336 0.3053576
## 2   A.2 real       0  3703      0      0   0 14.28 0.21301456 1.2905752
## 3   A.3 real       0  2713      0      0   0  5.10 0.28065638 0.5041429
## 4   A.4 real       0  4554      0      0   0 42.81 0.06542491 1.3951514
## 5   A.5 real       0  2853      0      0   0 10.00 0.31222343 0.6725128
## 6   A.6 real       0  3602      0      0   0  5.88 0.09590089 0.2738241
##   Cardinality
## 1          NA
## 2          NA
## 3          NA
## 4          NA
## 5          NA
## 6          NA</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">h2o.table</span>(spam.h2o<span class="op">$</span>spam)</code></pre>
<pre><code>##    spam Count
## 1 email  2788
## 2  spam  1813
## 
## [2 rows x 2 columns]</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Split to train and test</span>
splits &lt;-<span class="st"> </span><span class="kw">h2o.splitFrame</span>(<span class="dt">data =</span> spam.h2o, <span class="dt">ratios =</span> <span class="kw">c</span>(<span class="fl">0.8</span>))
train &lt;-<span class="st"> </span>splits[[<span class="dv">1</span>]]
test &lt;-<span class="st"> </span>splits[[<span class="dv">2</span>]]

<span class="co"># Fit a random forest</span>
rf &lt;-<span class="st"> </span><span class="kw">h2o.randomForest</span>(
  <span class="dt">x =</span> <span class="kw">names</span>(spam.h2o)[<span class="op">-</span><span class="dv">58</span>],
  <span class="dt">y =</span> <span class="kw">c</span>(<span class="st">&quot;spam&quot;</span>),
  <span class="dt">training_frame =</span> train,
  <span class="dt">model_id =</span> <span class="st">&quot;our.rf&quot;</span>)

<span class="co"># Predict on test set</span>
predictions &lt;-<span class="st"> </span><span class="kw">h2o.predict</span>(rf, test)
<span class="kw">head</span>(predictions)</code></pre>
<pre><code>##   predict     email      spam
## 1    spam 0.0600000 0.9400000
## 2    spam 0.2196667 0.7803333
## 3    spam 0.1111488 0.8888512
## 4    spam 0.0000000 1.0000000
## 5    spam 0.0500000 0.9500000
## 6   email 0.6989340 0.3010660</code></pre>
<p>Things to note:</p>
<ul>
<li>We did not install the H2O server; <code>install.packages(&quot;h2o&quot;)</code> did it for us.</li>
<li><code>h2o.init</code> fires the H2O server. Use <code>nthreads</code> to manually control the number of threads, or use the defaults.</li>
<li>H2O objects behave a lot like data.frame/tables.</li>
<li>To compute on H2O objects, you need dedicated function. They typically start with “h2o” such as <code>h2o.table</code>, and <code>h2o.randomForest</code>.</li>
<li><code>h2o.randomForest</code>, and other H2O functions, have their own syntax with many many options. Make sure to read <code>?h2o.randomForest</code>.</li>
</ul>
<div id="sparkling" class="section level4">
<h4><span class="header-section-number">16.4.4.1</span> Sparkling-Water</h4>
<p>The <em>h2o</em> package (<a href="parallel.html#h2o">16.4.4</a>) works with <code>H2OFrame</code> class objects.
If your data is stored in Spark, it may be more natural to work with <em>Spark DataFrames</em> instead of <code>H2OFrame</code>s.
This is exactly the purpose of the <a href="https://www.h2o.ai/products/h2o-sparkling-water/">Sparkling-Water</a> system.
R users can connect to it using the <a href="http://docs.h2o.ai/sparkling-water/2.2/latest-stable/doc/rsparkling.html">RSparkling</a> package, written and maintained by H2O.</p>
</div>
</div>
</div>
<div id="nested-parallel" class="section level2">
<h2><span class="header-section-number">16.5</span> Caution: Nested Parallelism</h2>
<p>A common problem when parallelising is that the cores/machines you invoked explicitely, invoke themselves other cores/machines.
Consider a user <em>forking</em> multiple processes, which call each <code>data.table</code> itself invoking parallel processing.
This is called <em>nested parallelism</em>, which may cause you to lose control of the number of machine being invoked.
The operating system will spend most of its time with house keeping, instead of doing your computations.</p>
<p>Luckily, <code>data.table</code> was designed to avoid this, but it may happen in general.
If you are parallelising your linear algebra with OpenBLAS, you may control nested parallelism with the package <a href="https://cran.r-project.org/package=RhpcBLASctl">RhpcBLASctl package</a>.
In other cases, you should be aware of this, and may need to consult an expert.</p>
</div>
<div id="bibliographic-notes-14" class="section level2">
<h2><span class="header-section-number">16.6</span> Bibliographic Notes</h2>
<p>To understand how computers work in general, see <span class="citation">Bryant and O’Hallaron (<a href="#ref-bryant2015computer">2015</a>)</span>.
For a brief and excellent explanation on parallel computing in R see <span class="citation">Schmidberger et al. (<a href="#ref-schmidberger2009state">2009</a>)</span>.
For a full review see <span class="citation">Chapple et al. (<a href="#ref-chapple2016mastering">2016</a>)</span>.
For a blog-level introduction see <a href="http://www.parallelr.com/r-with-parallel-computing/">ParallelR</a>.
For an up-to-date list of packages supporting parallel programming see the High Performance Computing <a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html">R task view</a>.
For some theory of distributed machine learning, see <span class="citation">J. D. Rosenblatt and Nadler (<a href="#ref-rosenblatt2016optimality">2016</a>)</span>.</p>
<p>An excellent video explaining <code>data.table</code> and H2O, by the author of `data.table, is <a href="https://www.youtube.com/watch?v=5X7h1rZGVs0">this</a>.
More benchmarks in <a href="https://h2oai.github.io/db-benchmark/">here</a>.
More on Spark with R inthe book <a href="https://therinspark.com">Mastering Apache Spark with R</a>.</p>
<p>For a blog level introduction to linear algebra in R see <a href="https://blog.revolutionanalytics.com/2013/08/r-and-linear-algebra.html">Joseph Rickert’s entry</a>.
For a detailed discussion see <span class="citation">Oancea, Andrei, and Dragoescu (<a href="#ref-oancea2015accelerating">2015</a>)</span>.</p>
</div>
<div id="practice-yourself-12" class="section level2">
<h2><span class="header-section-number">16.7</span> Practice Yourself</h2>
<p>Try DataCamp’s <a href="https://www.datacamp.com/courses/parallel-programming-in-r">Parallel Programming in R</a>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-foreach">
<p>Analytics, Revolution, and Steve Weston. 2015. <em>Foreach: Provides Foreach Looping Construct for R</em>. <a href="https://CRAN.R-project.org/package=foreach">https://CRAN.R-project.org/package=foreach</a>.</p>
</div>
<div id="ref-bryant2015computer">
<p>Bryant, Randal E, and David R O’Hallaron. 2015. <em>Computer Systems: A Programmer’s Perspective Plus Masteringengineering with Pearson eText–Access Card Package</em>. Pearson.</p>
</div>
<div id="ref-chapple2016mastering">
<p>Chapple, Simon R, Eilidh Troup, Thorsten Forster, and Terence Sloan. 2016. <em>Mastering Parallel Programming with R</em>. Packt Publishing Ltd.</p>
</div>
<div id="ref-kane2013scalable">
<p>Kane, Michael J, John Emerson, Stephen Weston, and others. 2013. “Scalable Strategies for Computing with Massive Data.” <em>Journal of Statistical Software</em> 55 (14): 1–19.</p>
</div>
<div id="ref-maechler20062nd">
<p>Maechler, Martin, and Douglas Bates. 2006. “2nd Introduction to the Matrix Package.” <em>R Core Development Team. Accessed on: Https://Stat. Ethz. Ch/R-Manual/R-Devel/Library/Matrix/Doc/Intro2Matrix. Pdf</em>.</p>
</div>
<div id="ref-oancea2015accelerating">
<p>Oancea, Bogdan, Tudorel Andrei, and Raluca Mariana Dragoescu. 2015. “Accelerating R with High Performance Linear Algebra Libraries.” <em>arXiv Preprint arXiv:1508.00688</em>.</p>
</div>
<div id="ref-rosenblatt2016optimality">
<p>Rosenblatt, Jonathan D, and Boaz Nadler. 2016. “On the Optimality of Averaging in Distributed Statistical Learning.” <em>Information and Inference: A Journal of the IMA</em> 5 (4). Oxford University Press: 379–404.</p>
</div>
<div id="ref-schmidberger2009state">
<p>Schmidberger, Markus, Martin Morgan, Dirk Eddelbuettel, Hao Yu, Luke Tierney, and Ulrich Mansmann. 2009. “State of the Art in Parallel Computing with R.” <em>Journal of Statistical Software</em> 47 (1).</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="memory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="algebra.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/82-parallel.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Rcourse.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
