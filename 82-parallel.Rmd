# Parallel Computing {#parallel}

You would think that because you have an expensive multicore computer your computations will speed up. 
Well, no; unless you actively make sure of that. 
By default, no matter how many cores you have, the operating system will allocate each R session to a single core. 

To parallelise computations, we need to distinguish between two types of parallelism:

1. __Explicit parallelism__: where the user handles the parallelisation. 
1. __Implicit parallelism__: where the parallelisation is abstracted away from the user.

Clearly, implicit parallelism is more desirable.
It is, however, very hard to design software that can parallelise any algorithm, while adapting to your hardware, operating system, and other the software running on your device. 
A lot of parallelisation still has to be explicit, but stay tuned for technologies like [Ray](https://rise.cs.berkeley.edu/projects/ray/), [Apache Spark](https://spark.apache.org), [Apache Flink](https://flink.apache.org), [Chapel](https://chapel-lang.org), [PyTorch](https://pytorch.org), and others, which are making great advances in handling parallelism for you. 
Before we can understand what those do, we start with explicit parallelism. 


## When and How to Parallelise?

Your computations are too slow.
Should you store your data differently? 
Should you buy more RAM?
Should you parallelise locally? 
Should you go cloud?

Unlike what some vendors will make you think, there is no one-size-fits-all solution to speed problems. 
Solving a RAM bottleneck may consume more CPU. 
Solving a CPU bottleneck may consume more RAM.
Parallelisation means using multiple CPUs simultaneously. 
It will thus aid with CPU bottlenecks, but may consume more RAM.
Parallelising is thus ill advised when dealing with a RAM bottleneck. 
Memory bottlenecks are released with sparsity (Chapter \@ref(sparse)), or efficient memory usage (Chapter \@ref(memory)).

When deciding if, and how, to parallelise, it is crucial that you diagnose your bottleneck. 
The good news is- that diagnostics is not too hard.
Here are a few pointers:

1. Always have a system monitor open. Windows users have their [Task Manager](https://en.wikipedia.org/wiki/Task_Manager_(Windows)); Linux users have [top](https://en.wikipedia.org/wiki/Top_(software)), or preferably, [htop](https://en.wikipedia.org/wiki/Htop); Mac users have the [Activity Monitor](https://www.howtogeek.com/227240/how-to-monitor-your-macs-health-with-activity-monitor/). The system monitor will inform you how many CPUs are being used, and how much RAM is being used. 

1. If you escape your computation, and R takes a long time to respond, you are probably dealing with a RAM bottleneck. 

1. Profile your code to detect how much RAM and CPU are consumed by each line of code. See Hadley's [guide](http://adv-r.had.co.nz/Profiling.html).


In the best possible scenario, the number of operations you can perform scales with the number of processors: $$time * processors = operations$$. 
This is called _perfect scaling_. 
It is rarely observed in practice, since parallelizing incurs some computational overhead: setting up environments, copying memory, ...
For this reason, the typical speedup is sub-linear. 
Computer scientists call this [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law).


## Terminology

Here are some terms we will be needing.

### Hardware:

- __Cluster:__ A collection of interconnected computers.
- __Node/Machine:__ A single physical machine in the cluster. Components of a single node do not communicate via the cluster's network, but rather, via the node's circuitry. 
- __Processor/Socket/CPU:__
- __RAM:__ Random Access Memory. One of many types of memory in a computer. Possibly the most relevant type of memory when computing with data. 
- __GPU:__ Graphical Processing Unit. A computing unit, separate from the CPU. Originally dedicated to graphics and gaming, thus its name. Currently, GPUs are extremely popular for fitting and servicing Deep Neural Networks. 
- __TPU:__ Tensor Processing Unit. A computing unit, dedicated to servicing and fitting Deep Neural Networks.


### Software:

- __Process:__ A sequence of instructions in memory, with accompanying data. Various processes typically see different locations of memory. Interpreted languages like R, and Python operate on processes.
- __Thread:__ A sub-sequence of instructions, within a process. Various threads in a process may see the same memory. Compiled languages like C, C++, may operate on threads. 
 

### Types of Parallelism

- __Data Parallel:__ When performing differest tasks on the same data. 
- __Task Parallel:__ When performing the same task, on different data. 

Flynn's Taxonomy:



## Explicit Parallelism

R provides many frameworks for explicit parallelism.
Any such framework will include the means for starting R (salve/worker) sessions, and the means for communicating between these sessions. 
Except for developpers, a typical user will probably be using some high-level R package which will abstract away these stages. 
A communication protocol because different R sessions will be communication one with the other. An abstraction layer because providing the user will all the features and generality of the communication layer is needlessly cumbersome.

Starting new R processes, a.k.a. slaves:

- __Fork__: a mechanism, unique to Unix and Linux, that clones a process with accompanying instructions and data. Recall that processes do not share memory. Each process updates its own copy of the data. 

- __Spawn__: a machanism, available under all operating systems, for a process to start an "offspring" process. It is quite similar to _fork_.


Inter-process communication standards:

- __Parallel Virtual Machine__ (PVM): a communication protocol and software, developed the University of Tennessee, Oak Ridge National Laboratory and Emory University, and first released in 1989. Runs on Windows and Unix, thus allowing to compute on clusters running these two operating systems. Noways, it is mostly replaced by MPI. The same group responsible for PVM will later deliver _Programming with Big Data in R_ ([pbdR](https://pbdr.org)): a whole ecosystem of packages for running R on large computing clusters. 

- __Message Passing Interface__ (MPI): A communication protocol that has become the de-facto standard for communication in large distributed clusters. Particularly, for heterogeneous computing clusters with varying operating systems and hardware. The protocol has various software implementations such as [OpenMPI](https://en.wikipedia.org/wiki/Open_MPI) and [MPICH](https://en.wikipedia.org/wiki/MPICH), [Deino](http://mpi.deino.net/), [LAM/MPI](https://en.wikipedia.org/wiki/LAM/MPI). Interestingly, large computing clusters use MPI, while modern BigData analysis platforms such as Spark, and Ray do not. Why is this? See Jonathan Dursi's excellent but controversial [blog post](https://www.dursi.ca/post/hpc-is-dying-and-mpi-is-killing-it.html).

- __NetWorkSpaces__ (NWS):

- __Sockets__: 







### parallel
The __parallel__ package, maintained by the R-core team, was introduced in 2011 to unify two popular parallisation packages: __snow__ and __multicore__.
The __multicore__ package was designed to parallelise using the _fork_ mechanism, on Linux machines. 
The __snow__ package was designed to parallise using the _spawn_ echanism, on all operating systems. 
Servers/R-sessions started with __snow__ will not see the parent's data, which will have to be copied to sapwned sessions. 
At least there is less data redundancy. 
Spawning, unlike forking, can be done on remote machines, which communicate using MPI.





### Foreach 

For reasons detailed in @kane2013scalable, we recommend the __foreach__ parallelisation package [@foreach]. 
It allows us to: 

1. Decouple between our parallel algorithm and the parallelisation mechanism: we write parallelisable code once, and can later switch between _fork_, _MPI_, _VPM_, _NWS_, mechanisms. Future mechanism may also be supported. 

1. Combine with the `big.matrix` object from Chapter \@ref(memory) for _shared memory parallelisation_: all the machines may see the same data, so that we don't need to export objects from machine to machine. 


What do we mean by "switch the underlying parallelisation mechanism"? 
It means there are several packages that will handle communication between machines. 
Some are very general and will work on any cluster. 
Some are more specific and will work only on a single multicore machine (not a cluster) with a particular operating system.
These mechanisms include __multicore__, __snow__, __parallel__, and __Rmpi__.
The compatibility between these mechanisms and __foreach__ is provided by another set of packages:
__doMC__ , __doMPI__, __doRedis__, __doParallel__, and __doSNOW__.


```{remark}
I personally prefer the __multicore__ mechanism, with the __doMC__ adapter for __foreach__.
I will not use this combo, however, because __multicore__ will not work on Windows machines.
I will thus use the more general __snow__ and __doParallel__ combo. 
If you do happen to run on Linux, or Unix, you will want to replace all __doParallel__ functionality with __doMC__.
```


Let's start with a simple example, taken from ["Getting Started with doParallel and foreach"](http://debian.mc.vanderbilt.edu/R/CRAN/web/packages/doParallel/vignettes/gettingstartedParallel.pdf).

```{r}
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)
result <- foreach(i=1:3) %dopar% sqrt(i)
class(result)
result
```

Things to note:

- `makeCluster` creates an object with the information our cluster. 
On a single machine it is very simple. On a cluster of machines, you will need to specify the i.p. addresses or other identifiers of the machines. 
- `registerDoParallel` is used to inform the __foreach__ package of the presence of our cluster. 
- The `foreach` function handles the looping. In particular note the `%dopar` operator that ensures that looping is in parallel. `%dopar%` can be replaced by `%do%` if you want serial looping (like the `for` loop), for instance, for debugging. 
- The output of the various machines is collected by `foreach` to a list object. 
- In this simple example, no data is shared between machines so we are not putting the shared memory capabilities to the test. 
- We can check how many workers were involved using the `getDoParWorkers()` function.
- We can check the parallelisation mechanism used with the `getDoParName()` function.


Here is a more involved example.
We now try to make [Bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) inference on the coefficients of a logistic regression.
Bootstrapping means that in each iteration, we resample the data, and refit the model. 

```{r}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- 1e4
ptime <- system.time({
 r <- foreach(icount(trials), .combine=cbind) %dopar% {
 ind <- sample(100, 100, replace=TRUE)
 result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
 coefficients(result1)
 }
 })[3]
ptime
```

Things to note:

- As usual, we use the `foreach` function with the `%dopar%` operator to loop in parallel.
- The `icounts` function generates a counter. 
- The `.combine=cbind` argument tells the `foreach` function how to combine the output of different machines, so that the returned object is not the default list. 


How long would that have taken in a simple (serial) loop? 
We only need to replace `%dopar%` with `%do%` to test.

```{r}
stime <- system.time({
 r <- foreach(icount(trials), .combine=cbind) %do% {
 ind <- sample(100, 100, replace=TRUE)
 result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
 coefficients(result1)
 }
 })[3]
stime
```

Yes. Parallelising is clearly faster.


Let's see how we can combine the power of __bigmemory__ and __foreach__ by creating a file mapped `big.matrix` object, which is shared by all machines.
The following example is taken from @kane2013scalable, and uses the `big.matrix` object we created in Chapter \@ref(memory).

```{r}
library(bigmemory)
x <- attach.big.matrix("airline.desc")

library(foreach)
library(doSNOW)
cl <- makeSOCKcluster(rep("localhost", 4)) # make a cluster of 4 machines
registerDoSNOW(cl) # register machines for foreach()
```

Get a "description" of the `big.matrix` object that will be used to call it from each machine.
```{r}
xdesc <- describe(x) 
```

Split the data along values of `BENE_AGE_CAT_CD`.
```{r}
G <- split(1:nrow(x), x[, "BENE_AGE_CAT_CD"]) 
```

Define a function that computes quantiles of `CAR_LINE_ICD9_DGNS_CD`.
```{r}
GetDepQuantiles <- function(rows, data) {
 quantile(data[rows, "CAR_LINE_ICD9_DGNS_CD"], probs = c(0.5, 0.9, 0.99),
 na.rm = TRUE)
}
```

We are all set up to loop, in parallel, and compute quantiles of `CAR_LINE_ICD9_DGNS_CD` for each value of `BENE_AGE_CAT_CD`.

```{r}
qs <- foreach(g = G, .combine = rbind) %dopar% {
 require("bigmemory")
 x <- attach.big.matrix(xdesc)
 GetDepQuantiles(rows = g, data = x)
}
qs
```




### Rdsm


### pbdR
Not on a single machine. 
https://pbdr.org

The pbdMPI package provides S4 classes to directly interface MPI in order to support the Single Program/Multiple Data (SPMD) parallel programming style which is particularly useful for batch parallel execution. The pbdSLAP builds on this and uses scalable linear algebra packages (namely BLACS, PBLAS, and ScaLAPACK) in double precision based on ScaLAPACK version 2.0.2. The pbdBASE builds on these and provides the core classes and methods for distributed data types upon which the pbdDMAT builds to provide distributed dense matrices for "Programming with Big Data". The pbdNCDF4 package permits multiple processes to write to the same file (without manual synchronization) and supports terabyte-sized files. The pbdDEMO package provides examples for these packages, and a detailed vignette. The pbdPROF package profiles MPI communication SPMD code via MPI profiling libraries, such as fpmpi, mpiP, or TAU.






## Implicit Parallelism

### Parallel Linear Algebra

- You can enjoy parallel linear algebra by replacing the linear algebra libraries with BLAS and LAPACK as described [here](https://www.r-bloggers.com/faster-r-through-better-blas/).
- You should read the "Parallel computing: Implicit parallelism" section in the excellent [High Performance Computing](https://cran.r-project.org/web/views/HighPerformanceComputing.html) task view, for the latest developments in implicit parallelism.




```{r}
n <- 4*1024
A <- matrix( rnorm(n*n), ncol=n, nrow=n )
B <- matrix( rnorm(n*n), ncol=n, nrow=n )
C <- A %*% B
```

![top at 657\% CPU!](art/Screenshot from 2019-09-28 20-25-48)


In my system I have installed [OpenBLAS](https://en.wikipedia.org/wiki/OpenBLAS):
```{r}
extSoftVersion()["BLAS"]
```




### Caution: Implicit with Explicit Parallelism

A common problem when parallelising is that the machines you invoked with __explicit__ parallelism, invoke other machines using __implicit__ parallelism. 
You then lose control of the number of machine being invoked, and the operating system spends most of its time managing resources, instead of doing your computations. 

Modern linear algebra libraries may try to implicitly parallelise vector and matrix operations.
To avoid this type of implicit parallelism within an explicit parallelisation, use the [RhpcBLASctl package](), and in particular  _evaRhpcBLASctl::blas_set_num_threads()_.
In other cases, you should consult an expert. 


### Parallel Data Munging with data.table


### Parallel Learning from a Centralized File System


### Parallel Learning from a Distributed File System


#### Spark










## Bibliographic Notes
For a brief and excellent explanation on parallel computing in R see @schmidberger2009state.
For a full review see @chapple2016mastering.
For a blog-level introduction see [ParallelR
](http://www.parallelr.com/r-with-parallel-computing/).
For an up-to-date list of packages supporting parallel programming see the High Performance Computing [R task view](https://cran.r-project.org/web/views/HighPerformanceComputing.html).
To understand how computers work in general, see @bryant2015computer.

## Practice Yourself

Try DataCamp's [Parallel Programming in R](https://www.datacamp.com/courses/parallel-programming-in-r).
